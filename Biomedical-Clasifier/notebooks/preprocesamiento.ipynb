{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a80fc3f",
   "metadata": {},
   "source": [
    "## preprocesamiento dataset\n",
    "\n",
    "\n",
    "Une el texto: Primero, toma el título y el resumen de cada investigación y los junta en un solo párrafo coherente.\n",
    "\n",
    "Estandariza las categorías: Luego, revisa la columna que dice a qué especialidad médica pertenece cada investigación (ej. \"Oncología\", \"cardiología\", etc.) y la limpia para que todo esté en minúsculas y sin espacios raros. Así, se asegura de que no haya confusiones.\n",
    "\n",
    "Traduce a números: Esta es la parte clave. Crea nuevas columnas, una para cada especialidad médica importante. Luego, para cada investigación, marca con un 1 si pertenece a esa especialidad y con un 0 si no. Esto es como pasar una lista de asistencia, marcando \"presente\" o \"ausente\" para cada categoría.\n",
    "\n",
    "Guarda el resultado final: Finalmente, se queda solo con lo importante (el párrafo de texto y las nuevas columnas de 1s y 0s) y lo guarda todo en un archivo nuevo, ya limpio y listo para ser usado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ff312f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\raw\\\\challenge_data-18-ago.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Cargar dataset original\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mraw\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mchallenge_data-18-ago.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m;\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Función robusta para unir título y abstract\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munir_texto\u001b[39m(title, abstract):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data\\\\raw\\\\challenge_data-18-ago.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar dataset original\n",
    "df = pd.read_csv(r\"data\\raw\\challenge_data-18-ago.csv\", sep=\";\")\n",
    "\n",
    "# Función robusta para unir título y abstract\n",
    "def unir_texto(title, abstract):\n",
    "    title = str(title).strip()   # quitar espacios en los extremos\n",
    "    abstract = str(abstract).strip()\n",
    "    if title.endswith(\".\"):      # si ya tiene punto final\n",
    "        return title + \" \" + abstract\n",
    "    else:                        # si no lo tiene, se lo agregamos\n",
    "        return title + \". \" + abstract\n",
    "\n",
    "# Crear columna text\n",
    "df[\"text\"] = df.apply(lambda row: unir_texto(row[\"title\"], row[\"abstract\"]), axis=1)\n",
    "\n",
    "# Normalizamos categorías\n",
    "df[\"group\"] = df[\"group\"].str.lower().str.strip()\n",
    "\n",
    "# Crear columnas multietiqueta\n",
    "for label in [\"cardiovascular\", \"hepatorenal\", \"neurological\", \"oncological\"]:\n",
    "    df[label] = df[\"group\"].apply(lambda x: 1 if label in x else 0)\n",
    "\n",
    "# Nos quedamos solo con lo necesario\n",
    "df_final = df[[\"text\", \"cardiovascular\", \"hepatorenal\", \"neurological\", \"oncological\"]]\n",
    "\n",
    "# Exportar\n",
    "df_final.to_csv(\"data\\processed\\dataset_preprocesado.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Archivo procesado y guardado en data\\processed\\dataset_preprocesado.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ac3e8",
   "metadata": {},
   "source": [
    "## Separacion en entrenamiento, validacion, prueba\n",
    "En resumen, este código es un planificador estratégico para dividir los datos. Su objetivo es asegurarse de que, cuando separemos los datos para entrenar y probar la inteligencia artificial, la división sea justa e inteligente, especialmente con los casos más raros.\n",
    "\n",
    "Esto es lo que hace paso a paso:\n",
    "\n",
    "Analiza las Combinaciones: Primero, lee el archivo de datos ya limpio. En lugar de mirar cada enfermedad por separado, se fija en las combinaciones de enfermedades que tiene cada investigación. Por ejemplo, identifica si un estudio trata solo de \"cáncer\", o si trata de \"cáncer + neurología\" al mismo tiempo. A cada combinación le pone una etiqueta única.\n",
    "\n",
    "Cuenta los Grupos: Una vez que ha etiquetado todas las investigaciones, cuenta cuántos ejemplos hay de cada combinación. Básicamente, hace un inventario para saber qué tan comunes o raras son. Por ejemplo, podría descubrir que hay 500 estudios solo de \"cáncer\" pero solo 3 que son de \"cáncer + neurología\".\n",
    "\n",
    "Crea un Plan de Reparto Inteligente (80/10/10): Esta es la parte más importante. Sabiendo cuántos ejemplos hay de cada tipo, diseña un plan para dividirlos en tres montones:\n",
    "\n",
    "Entrenamiento (80%): El montón más grande, para que el modelo aprenda.\n",
    "Validación (10%): Un pequeño montón para hacer pruebas durante el entrenamiento.\n",
    "Prueba (10%): El montón final para ver qué tan bueno es el modelo.\n",
    "La clave es que lo hace de forma muy cuidadosa. Si una combinación de enfermedades es muy rara (por ejemplo, solo hay 2 o 3 casos), se asegura de no \"romper\" ese grupito, poniéndolos juntos en el montón de entrenamiento para que el modelo al menos pueda aprender de ellos. Para las combinaciones más comunes, sí las reparte en los tres montones siguiendo la proporción 80/10/10.\n",
    "\n",
    "Al final, lo que te muestra en pantalla no son los datos divididos todavía, sino el plan detallado de cómo se van a dividir, garantizando que cada combinación, por rara que sea, esté representada de forma justa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ecb3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 1: Archivo 'dataset_preprocesado.csv' cargado exitosamente.\n",
      "Paso 1: Distribución de combinaciones calculada.\n",
      "\n",
      "Paso 2: Plan de división 80/10/10 generado:\n",
      "                                                 domain  count  train_count  val_count  test_count\n",
      "0                                          neurological   1058          846        106         106\n",
      "1                                        cardiovascular    645          517         64          64\n",
      "2                                           hepatorenal    533          427         53          53\n",
      "3                           cardiovascular+neurological    308          246         31          31\n",
      "4                                           oncological    237          189         24          24\n",
      "5                              hepatorenal+neurological    202          162         20          20\n",
      "6                            cardiovascular+hepatorenal    190          152         19          19\n",
      "7                              neurological+oncological    143          115         14          14\n",
      "8                               hepatorenal+oncological     98           78         10          10\n",
      "9                            cardiovascular+oncological     70           56          7           7\n",
      "10              cardiovascular+hepatorenal+neurological     28           22          3           3\n",
      "11                 hepatorenal+neurological+oncological     26           20          3           3\n",
      "12              cardiovascular+neurological+oncological     13           11          1           1\n",
      "13               cardiovascular+hepatorenal+oncological      7            5          1           1\n",
      "14  cardiovascular+hepatorenal+neurological+oncological      7            5          1           1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- PASO 1: Leer el CSV y calcular la distribución de combinaciones ---\n",
    "\n",
    "try:\n",
    "    df_raw = pd.read_csv('data/dataset_preprocesado.csv')\n",
    "    print(\"Paso 1: Archivo 'dataset_preprocesado.csv' cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: El archivo 'dataset_preprocesado.csv' no fue encontrado.\")\n",
    "    exit()\n",
    "\n",
    "# Columnas que representan las enfermedades\n",
    "disease_columns = ['cardiovascular', 'hepatorenal', 'neurological', 'oncological']\n",
    "\n",
    "# Función para crear la cadena de combinación para cada fila\n",
    "def create_domain_string(row):\n",
    "    present_diseases = [col for col in disease_columns if row[col] == 1]\n",
    "    if present_diseases:\n",
    "        return '+'.join(present_diseases)\n",
    "    return 'none' # Etiqueta para filas sin ninguna de estas enfermedades\n",
    "\n",
    "# Crear una nueva columna 'domain' con la combinación de enfermedades\n",
    "df_raw['domain'] = df_raw.apply(create_domain_string, axis=1)\n",
    "\n",
    "# Calcular la distribución (excluyendo las que no tienen ninguna enfermedad)\n",
    "distribution_counts = df_raw[df_raw['domain'] != 'none']['domain'].value_counts().reset_index()\n",
    "distribution_counts.columns = ['domain', 'count']\n",
    "print(\"Paso 1: Distribución de combinaciones calculada.\\n\")\n",
    "\n",
    "\n",
    "# --- PASO 2: Crear el plan de división (80/10/10) ---\n",
    "\n",
    "def split_data_counts(row):\n",
    "    \"\"\"Calcula cuántas muestras van a train/val/test para una fila de la tabla de distribución.\"\"\"\n",
    "    count = row['count']\n",
    "    if count == 1:\n",
    "        return pd.Series([1, 0, 0], index=['train_count', 'val_count', 'test_count'])\n",
    "    elif count == 2:\n",
    "        return pd.Series([1, 1, 0], index=['train_count', 'val_count', 'test_count'])\n",
    "    elif count == 3:\n",
    "        return pd.Series([1, 1, 1], index=['train_count', 'val_count', 'test_count'])\n",
    "    else:\n",
    "        val_count = max(1, int(np.round(count * 0.1)))\n",
    "        test_count = max(1, int(np.round(count * 0.1)))\n",
    "        train_count = count - val_count - test_count\n",
    "        return pd.Series([train_count, val_count, test_count], index=['train_count', 'val_count', 'test_count'])\n",
    "\n",
    "# Aplicar la función para obtener el plan de división\n",
    "split_plan = distribution_counts.copy()\n",
    "split_counts = split_plan.apply(split_data_counts, axis=1)\n",
    "split_plan = pd.concat([split_plan, split_counts], axis=1)\n",
    "\n",
    "print(\"Paso 2: Plan de división 80/10/10 generado:\")\n",
    "print(split_plan.to_string())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83002711",
   "metadata": {},
   "source": [
    "## ejecucion de la separacion del dataset\n",
    "\n",
    "En resumen, si el código anterior era el \"planificador\", este código es el \"ejecutor\". Toma el plan que se creó y lo lleva a cabo, repartiendo físicamente cada dato en el montón que le corresponde.\n",
    "\n",
    "Así es como lo hace, paso a paso:\n",
    "\n",
    "Repartir los Datos, Grupo por Grupo:\n",
    "\n",
    "El código revisa el plan, fila por fila. Cada fila corresponde a una combinación de enfermedades (ej. \"solo cáncer\", o \"cáncer + neurología\").\n",
    "Para cada combinación, busca todos los estudios que pertenecen a ese grupo en la tabla de datos original.\n",
    "Luego, baraja aleatoriamente ese pequeño grupo de estudios. Esto es como barajar una parte de la baraja para que el reparto sea justo.\n",
    "Finalmente, \"corta\" ese grupo barajado según los números del plan: los primeros van al montón de entrenamiento, los siguientes al de validación, y los últimos al de prueba.\n",
    "Repite este proceso para todas y cada una de las combinaciones de enfermedades hasta que no queda ningún dato sin asignar.\n",
    "Juntar y Mezclar los Montones Finales:\n",
    "\n",
    "Después del reparto, ahora tiene un montón de pequeños \"sub-grupos\" para entrenamiento, validación y prueba.\n",
    "Lo que hace a continuación es juntar todos los pedacitos de \"entrenamiento\" en un único y gran archivo de entrenamiento. Hace lo mismo para los otros dos.\n",
    "Para terminar, vuelve a barajar cada uno de los tres montones grandes. Esto es muy importante para que el modelo de inteligencia artificial aprenda de forma desordenada y no, por ejemplo, viendo todos los casos de cáncer juntos.\n",
    "Guardar los Resultados:\n",
    "\n",
    "Una vez que los tres conjuntos de datos (entrenamiento, validación y prueba) están listos, limpios y bien mezclados, los guarda en tres archivos CSV separados.\n",
    "Hacer una Última Verificación:\n",
    "\n",
    "Al final, simplemente hace una suma rápida. Cuenta cuántos datos había al principio y cuántos hay en los tres nuevos archivos sumados, para asegurarse de que no se perdió ninguna investigación en el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a84dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 3: Iniciando la división de datos por cada combinación...\n",
      "Paso 3: División completada.\n",
      "\n",
      "Paso 4: Archivos generados exitosamente:\n",
      "- train_set.csv\n",
      "- val_set.csv\n",
      "- test_set.csv\n",
      "\n",
      "--- Verificación Final ---\n",
      "Total de muestras originales: 3565\n",
      "Muestras en train_set.csv: 2851\n",
      "Muestras en val_set.csv:   357\n",
      "Muestras en test_set.csv:  357\n",
      "Suma total:                3565\n",
      "¡Proceso finalizado!\n"
     ]
    }
   ],
   "source": [
    "# --- PASO 3: Dividir el DataFrame original y crear los 3 conjuntos de datos ---\n",
    "\n",
    "# Listas para almacenar los dataframes de cada conjunto\n",
    "train_dfs, val_dfs, test_dfs = [], [], []\n",
    "\n",
    "print(\"Paso 3: Iniciando la división de datos por cada combinación...\")\n",
    "\n",
    "# Iterar sobre el plan de división\n",
    "for _, row in split_plan.iterrows():\n",
    "    domain = row['domain']\n",
    "    train_num = row['train_count']\n",
    "    val_num = row['val_count']\n",
    "    \n",
    "    # Filtrar el dataframe original por la combinación actual\n",
    "    domain_df = df_raw[df_raw['domain'] == domain]\n",
    "    \n",
    "    # Barajar aleatoriamente los datos de esta combinación para asegurar una división imparcial\n",
    "    shuffled_domain_df = domain_df.sample(frac=1, random_state=42) # random_state para reproducibilidad\n",
    "    \n",
    "    # Cortar y asignar los datos a los conjuntos correspondientes\n",
    "    train_slice = shuffled_domain_df.iloc[:train_num]\n",
    "    val_slice = shuffled_domain_df.iloc[train_num:train_num + val_num]\n",
    "    test_slice = shuffled_domain_df.iloc[train_num + val_num:]\n",
    "    \n",
    "    # Añadir las rebanadas a las listas\n",
    "    train_dfs.append(train_slice)\n",
    "    val_dfs.append(val_slice)\n",
    "    test_dfs.append(test_slice)\n",
    "\n",
    "# Combinar todas las rebanadas en tres DataFrames finales\n",
    "train_set = pd.concat(train_dfs)\n",
    "val_set = pd.concat(val_dfs)\n",
    "test_set = pd.concat(test_dfs)\n",
    "\n",
    "# Barajar los conjuntos finales para que las combinaciones no queden agrupadas\n",
    "train_set = train_set.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_set = val_set.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_set = test_set.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Eliminar la columna auxiliar 'domain' antes de guardar\n",
    "train_set = train_set.drop(columns=['domain'])\n",
    "val_set = val_set.drop(columns=['domain'])\n",
    "test_set = test_set.drop(columns=['domain'])\n",
    "\n",
    "print(\"Paso 3: División completada.\\n\")\n",
    "\n",
    "\n",
    "# --- PASO 4: Guardar los archivos CSV ---\n",
    "\n",
    "train_set.to_csv('data\\processed\\train_set.csv', index=False)\n",
    "val_set.to_csv('data\\processed\\val_set.csv', index=False)\n",
    "test_set.to_csv('data\\processed\\test_set.csv', index=False)\n",
    "\n",
    "print(\"Paso 4: Archivos generados exitosamente:\")\n",
    "print(\"- train_set.csv\")\n",
    "print(\"- val_set.csv\")\n",
    "print(\"- test_set.csv\\n\")\n",
    "\n",
    "# --- Verificación Final ---\n",
    "print(\"--- Verificación Final ---\")\n",
    "print(f\"Total de muestras originales: {len(df_raw[df_raw['domain'] != 'none'])}\")\n",
    "print(f\"Muestras en train_set.csv: {len(train_set)}\")\n",
    "print(f\"Muestras en val_set.csv:   {len(val_set)}\")\n",
    "print(f\"Muestras en test_set.csv:  {len(test_set)}\")\n",
    "print(f\"Suma total:                {len(train_set) + len(val_set) + len(test_set)}\")\n",
    "print(\"¡Proceso finalizado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c6902",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "En resumen, este código es un \"estratega de datos\". Su trabajo es analizar el conjunto de datos de entrenamiento, encontrar dónde hay desequilibrios (es decir, qué categorías tienen muy pocos ejemplos) y crear un plan detallado para solucionar ese problema generando datos nuevos. La técnica que planea usar es la \"paráfrasis\": tomar un artículo existente y reescribirlo para crear una nueva versión.\n",
    "\n",
    "Aquí tienes el desglose de su estrategia:\n",
    "\n",
    "Hacer un Inventario: Primero, abre el archivo de datos de entrenamiento y, una vez más, cuenta cuántos artículos hay para cada combinación específica de enfermedades. Este inventario es la base de toda su estrategia.\n",
    "\n",
    "Diseñar un Plan de \"Relleno\" en Tres Fases: El código es muy inteligente y no trata a todos los artículos por igual. Sigue un plan jerárquico para decidir qué artículos duplicar:\n",
    "\n",
    "Fase 1: Los más complejos y raros. Empieza con los artículos que cubren 3 o 4 enfermedades a la vez. Sabe que estos son muy valiosos. Su estrategia es nivelar los grupos: si la combinación más común de 3 enfermedades tiene 20 ejemplos, planea crear paráfrasis de las otras combinaciones de 3 enfermedades hasta que todas lleguen a 20.\n",
    "\n",
    "Fase 2: Los de en medio. Luego, hace lo mismo con los artículos que cubren 2 enfermedades. Busca la combinación más popular y planea \"rellenar\" las demás para que alcancen ese mismo nivel.\n",
    "\n",
    "Fase 3: El Gran Balance Final. Después de nivelar los grupos más complejos, mira el panorama general. Calcula el total de artículos para cada enfermedad individual (por ejemplo, cuántos estudios sobre \"cáncer\" hay en total, sin importar con qué esté combinado). Identifica la enfermedad con más ejemplos y establece ese número como el objetivo final para todas las demás. Luego, planea usar los artículos más simples (los de una sola enfermedad) como \"relleno\" para aumentar el conteo de las categorías que se quedaron atrás, hasta que todas estén equilibradas.\n",
    "\n",
    "Presentar el Plan de Acción y los Resultados Esperados:\n",
    "\n",
    "Al final, el código no ejecuta la creación de datos. En su lugar, te presenta el plan completo y listo para ejecutar. Te dice exactamente cuántos artículos de cada combinación específica necesitas parafrasear.\n",
    "También te da una proyección del \"antes y después\". Te muestra una tabla comparando la cantidad de datos que tienes ahora con la cantidad que tendrás si sigues el plan, demostrando que al final todo quedará perfectamente balanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d32071d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'train_set.csv' cargado correctamente.\n",
      "\n",
      "--- 📋 PLAN DE BALANCEO HÍBRIDO FINAL (4 paráfrasis/llamada) 📋 ---\n",
      "🔹 Parafrasear 163 artículos de la combinación: 'oncological'\n",
      "🔹 Parafrasear 104 artículos de la combinación: 'hepatorenal'\n",
      "🔹 Parafrasear 82 artículos de la combinación: 'cardiovascular'\n",
      "🔹 Parafrasear 48 artículos de la combinación: 'cardiovascular+oncological'\n",
      "🔹 Parafrasear 42 artículos de la combinación: 'hepatorenal+oncological'\n",
      "🔹 Parafrasear 33 artículos de la combinación: 'neurological+oncological'\n",
      "🔹 Parafrasear 24 artículos de la combinación: 'cardiovascular+hepatorenal'\n",
      "🔹 Parafrasear 21 artículos de la combinación: 'hepatorenal+neurological'\n",
      "🔹 Parafrasear 5 artículos de la combinación: 'cardiovascular+hepatorenal+neurological+oncological'\n",
      "🔹 Parafrasear 5 artículos de la combinación: 'cardiovascular+hepatorenal+oncological'\n",
      "🔹 Parafrasear 3 artículos de la combinación: 'cardiovascular+neurological+oncological'\n",
      "🔹 Parafrasear 1 artículos de la combinación: 'hepatorenal+neurological+oncological'\n",
      "\n",
      "-----------------------------------\n",
      "📞 Total de llamadas a la API necesarias: 531\n",
      "-----------------------------------\n",
      "\n",
      "--- 📈 RESULTADOS PROYECTADOS (TOTALES DE CATEGORÍA) 📈 ---\n",
      "Categoría       Inicial    Final     \n",
      "-----------------------------------\n",
      "Cardiovascular  1014       1682      \n",
      "Hepatorenal     871        1679      \n",
      "Neurological    1427       1679      \n",
      "Oncological     479        1679      \n",
      "\n",
      "--- 📊 RESULTADOS PROYECTADOS (DISTRIBUCIÓN DE COMBINACIONES) 📊 ---\n",
      "Combinación                                             Inicial    Final     \n",
      "---------------------------------------------------------------------------\n",
      "cardiovascular                                          517        845       \n",
      "hepatorenal                                             427        843       \n",
      "neurological                                            846        846       \n",
      "oncological                                             189        841       \n",
      "cardiovascular+hepatorenal                              152        248       \n",
      "cardiovascular+neurological                             246        246       \n",
      "cardiovascular+oncological                              56         248       \n",
      "hepatorenal+neurological                                162        246       \n",
      "hepatorenal+oncological                                 78         246       \n",
      "neurological+oncological                                115        247       \n",
      "cardiovascular+hepatorenal+neurological                 22         22        \n",
      "cardiovascular+hepatorenal+oncological                  5          25        \n",
      "cardiovascular+neurological+oncological                 11         23        \n",
      "hepatorenal+neurological+oncological                    20         24        \n",
      "cardiovascular+hepatorenal+neurological+oncological     5          25        \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# --- PASO 0: Leer el dataset de entrenamiento y calcular la distribución ---\n",
    "\n",
    "try:\n",
    "    # Cargar el conjunto de entrenamiento generado previamente\n",
    "    df_train = pd.read_csv('database/train_set.csv')\n",
    "    print(\"Archivo 'train_set.csv' cargado correctamente.\\n\")\n",
    "\n",
    "    # Columnas que representan las enfermedades\n",
    "    disease_columns = ['cardiovascular', 'hepatorenal', 'neurological', 'oncological']\n",
    "\n",
    "    # Función para crear la cadena de combinación para cada fila\n",
    "    def create_domain_string(row):\n",
    "        present_diseases = [col for col in disease_columns if row[col] == 1]\n",
    "        if present_diseases:\n",
    "            return '+'.join(present_diseases)\n",
    "        return 'none'\n",
    "\n",
    "    # Crear la columna 'domain' y calcular la distribución\n",
    "    df_train['domain'] = df_train.apply(create_domain_string, axis=1)\n",
    "    \n",
    "    # Convertir la serie de value_counts a un diccionario\n",
    "    combination_counts = df_train[df_train['domain'] != 'none']['domain'].value_counts().to_dict()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: No se encontró el archivo 'train_set.csv'.\")\n",
    "    print(\"Asegúrate de haber ejecutado el script anterior para generarlo.\")\n",
    "    exit()\n",
    "\n",
    "# --- A PARTIR DE AQUÍ, ES EL SCRIPT DE OPTIMIZACIÓN QUE PROPORCIONASTE ---\n",
    "\n",
    "# --- 1. DATOS INICIALES Y ESTRUCTURACIÓN ---\n",
    "strata = {i: {} for i in range(1, 5)}\n",
    "for combo, count in combination_counts.items():\n",
    "    num_labels = combo.count('+') + 1\n",
    "    strata[num_labels][combo] = count\n",
    "\n",
    "# --- 2. PASO 1: REGLA FIJA PARA ARTÍCULOS DE 4 ETIQUETAS ---\n",
    "paraphrase_plan = {combo: 0 for combo in combination_counts}\n",
    "\n",
    "if 4 in strata and strata[4]:\n",
    "    for combo, initial_count in strata[4].items():\n",
    "        paraphrase_plan[combo] = initial_count\n",
    "\n",
    "# --- 3. PASO 2: BALANCEAR ESTRATOS DE 3 Y 2 ETIQUETAS ---\n",
    "for i in [3, 2]:\n",
    "    stratum = strata.get(i, {})\n",
    "    if not stratum: continue\n",
    "    \n",
    "    target_count = max(stratum.values())\n",
    "    \n",
    "    for combo, initial_count in stratum.items():\n",
    "        if initial_count < target_count:\n",
    "            needed_articles = target_count - initial_count\n",
    "            calls_needed = math.ceil(needed_articles / 4)\n",
    "            paraphrase_plan[combo] += min(calls_needed, initial_count)\n",
    "\n",
    "# --- 4. CÁLCULO DE TOTALES INTERMEDIOS ---\n",
    "categories = [\"cardiovascular\", \"hepatorenal\", \"neurological\", \"oncological\"]\n",
    "intermediate_totals = {cat: 0 for cat in categories}\n",
    "for combo, count in combination_counts.items():\n",
    "    for cat in categories:\n",
    "        if cat in combo:\n",
    "            final_count = count + 4 * paraphrase_plan.get(combo, 0)\n",
    "            intermediate_totals[cat] += final_count\n",
    "\n",
    "# --- 5. PASO 3: USAR ARTÍCULOS DE 1 ETIQUETA COMO RELLENO ---\n",
    "target_total = max(intermediate_totals.values())\n",
    "\n",
    "for cat in categories:\n",
    "    deficit = target_total - intermediate_totals[cat]\n",
    "    if deficit > 0:\n",
    "        calls_needed = math.ceil(deficit / 4)\n",
    "        single_label_combo = cat\n",
    "        initial_count = combination_counts.get(single_label_combo, 0)\n",
    "        # Asegurarnos que el combo de una etiqueta exista antes de añadir al plan\n",
    "        if single_label_combo in paraphrase_plan:\n",
    "             paraphrase_plan[single_label_combo] += min(calls_needed, initial_count)\n",
    "        else:\n",
    "             paraphrase_plan[single_label_combo] = min(calls_needed, initial_count)\n",
    "\n",
    "# --- 6. MOSTRAR RESULTADOS FINALES ---\n",
    "final_plan = {k: v for k, v in paraphrase_plan.items() if v > 0}\n",
    "total_api_calls = sum(final_plan.values())\n",
    "\n",
    "print(\"--- 📋 PLAN DE BALANCEO HÍBRIDO FINAL (4 paráfrasis/llamada) 📋 ---\")\n",
    "sorted_plan = sorted(final_plan.items(), key=lambda item: item[1], reverse=True)\n",
    "for combo, calls in sorted_plan:\n",
    "    print(f\"🔹 Parafrasear {calls} artículos de la combinación: '{combo}'\")\n",
    "\n",
    "print(\"\\n-----------------------------------\")\n",
    "print(f\"📞 Total de llamadas a la API necesarias: {total_api_calls}\")\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 📈 RESULTADOS PROYECTADOS (TOTALES DE CATEGORÍA) 📈 ---\")\n",
    "final_totals = {cat: 0 for cat in categories}\n",
    "for combo, count in combination_counts.items():\n",
    "    for cat in categories:\n",
    "        if cat in combo:\n",
    "            final_totals[cat] += count + 4 * paraphrase_plan.get(combo, 0)\n",
    "\n",
    "print(f\"{'Categoría':<15} {'Inicial':<10} {'Final':<10}\")\n",
    "print(\"-\" * 35)\n",
    "for cat in categories:\n",
    "    initial_total = sum(c for co, c in combination_counts.items() if cat in co)\n",
    "    print(f\"{cat.capitalize():<15} {initial_total:<10} {final_totals[cat]:<10}\")\n",
    "\n",
    "print(\"\\n--- 📊 RESULTADOS PROYECTADOS (DISTRIBUCIÓN DE COMBINACIONES) 📊 ---\")\n",
    "print(f\"{'Combinación':<55} {'Inicial':<10} {'Final':<10}\")\n",
    "print(\"-\" * 75)\n",
    "# Asegurarnos de mostrar todas las combinaciones originales\n",
    "sorted_combos = sorted(combination_counts.keys(), key=lambda x: (x.count('+'), x))\n",
    "for combo in sorted_combos:\n",
    "    initial = combination_counts.get(combo, 0)\n",
    "    final = initial + 4 * paraphrase_plan.get(combo, 0)\n",
    "    print(f\"{combo:<55} {initial:<10} {final:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1c4059",
   "metadata": {},
   "source": [
    "## Generacion de datos con Gemini \n",
    "\n",
    "En resumen, si el código anterior era el \"estratega\", este es el \"operario de la fábrica\". Su única misión es ejecutar el plan de creación de datos, utilizando una potente inteligencia artificial (Gemini de Google) para hacer el trabajo pesado.\n",
    "\n",
    "Aquí está el proceso explicado de manera sencilla:\n",
    "\n",
    "Preparación y Verificación: Antes de encender las máquinas, el código hace una revisión de seguridad.\n",
    "\n",
    "Primero, se asegura de que el \"plan\" creado en la celda anterior esté disponible. Si no lo encuentra, detiene todo.\n",
    "Luego, busca la \"llave\" para acceder a la IA de Google (la API Key). Sin esta llave, no puede funcionar.\n",
    "Redactar las Instrucciones para la IA: Esta es una parte fundamental. El código crea una plantilla de instrucciones muy clara y estricta para la inteligencia artificial. En esencia, le dice:\n",
    "\n",
    "\"Actúa como un experto editor de textos médicos.\"\n",
    "\"Te daré un título y un resumen. Tu trabajo es reescribirlo 4 veces de formas diferentes.\"\n",
    "Reglas Cruciales: \"No te inventes nada, no cambies los datos, mantén un tono profesional y, lo más importante, entrégame cada versión en este formato exacto: Título reescrito. Resumen reescrito.\"\n",
    "\"Dame el resultado final empaquetado en un formato específico (JSON) para que yo lo pueda entender fácilmente.\"\n",
    "Poner en Marcha la Producción:\n",
    "\n",
    "El código se convierte en un \"jefe de producción\". Revisa el plan y, para cada combinación de enfermedades, toma la cantidad exacta de artículos que necesita parafrasear.\n",
    "Va uno por uno. Coge un artículo, lo mete en la plantilla de instrucciones y se lo envía a la IA de Gemini.\n",
    "Mientras la IA trabaja, muestra una barra de progreso para que puedas ver en tiempo real cómo avanza la creación de los nuevos datos.\n",
    "Cuando la IA devuelve las 4 nuevas versiones del texto, el código las recoge y las guarda en una lista temporal de \"datos nuevos\".\n",
    "Por cortesía, hace una pequeña pausa entre cada petición para no sobrecargar el servicio de la IA.\n",
    "Ensamblaje Final y Almacenamiento:\n",
    "\n",
    "Una vez que ha pasado por todo el plan y ha generado cientos de nuevos artículos, toma la lista de \"datos nuevos\".\n",
    "La combina con el conjunto de datos de entrenamiento original.\n",
    "El resultado es un nuevo archivo de entrenamiento mucho más grande y balanceado.\n",
    "Finalmente, guarda este archivo mejorado con el nombre train_set_expanded.csv, dejándolo listo para el paso final: entrenar un modelo mucho más robusto y preciso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb012c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Variables 'final_plan' y 'df_train' encontradas de la celda anterior.\n",
      "Se ejecutarán 531 llamadas a la API según el plan.\n",
      "🔑 API Key cargada desde la variable de entorno.\n",
      "🤖 Modelo seleccionado: models/gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🤖 Generando paráfrasis: 100%|██████████| 531/531 [2:17:43<00:00, 15.56s/it, Éxito: 'cardiovascular+hepatorena...']                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "¡Proceso completado! ✅\n",
      "Tamaño del dataset de entrenamiento original: 2851\n",
      "Número de muestras nuevas generadas: 2120\n",
      "Tamaño del dataset expandido: 4971\n",
      "Nuevo dataset guardado como: 'database/train_set_expanded.csv'\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SCRIPT DE EJECUCIÓN DEL PLAN DE BALANCEO\n",
    "# (Esta celda asume que 'final_plan' y 'df_train' existen de la celda anterior)\n",
    "# ==============================================================================\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv \n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "# --- 1. Verificación de Variables Previas ---\n",
    "try:\n",
    "    if 'final_plan' in locals() and 'df_train' in locals():\n",
    "        print(\"✅ Variables 'final_plan' y 'df_train' encontradas de la celda anterior.\")\n",
    "        total_calls = sum(final_plan.values())\n",
    "        print(f\"Se ejecutarán {total_calls} llamadas a la API según el plan.\")\n",
    "    else:\n",
    "        raise NameError\n",
    "except NameError:\n",
    "    print(\"❌ Error: Las variables 'final_plan' y 'df_train' no se encontraron.\")\n",
    "    print(\"Asegúrate de ejecutar la celda anterior que calcula el plan de optimización.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Configuración de la API ---\n",
    "load_dotenv() \n",
    "try:\n",
    "    API_KEY = os.environ.get('GEMINI_API_KEY')\n",
    "    if API_KEY is None: raise ValueError\n",
    "    print(\"🔑 API Key cargada desde la variable de entorno.\")\n",
    "except (ValueError, KeyError):\n",
    "    API_KEY = \"PEGA_TU_API_KEY_DE_GEMINI_AQUI\" # 🚨 REEMPLAZA ESTO\n",
    "    print(\"⚠️ API Key no encontrada. Usando la del script. ¡No olvides reemplazarla!\")\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-2.5-flash',\n",
    "    generation_config={\"response_mime_type\": \"application/json\"}\n",
    ")\n",
    "print(f\"🤖 Modelo seleccionado: {model.model_name}\")\n",
    "\n",
    "# --- 3. Definición del Prompt (MODIFICADO) ---\n",
    "prompt_template = \"\"\"\n",
    "Act as an expert medical and scientific text editor.\n",
    "Your task is to generate 4 distinct paraphrased versions of the provided research article title and abstract.\n",
    "\n",
    "Crucial Instructions:\n",
    "1. Preserve the factual meaning completely. Do not alter, invent, or omit any key medical entities, numerical results, or conclusions.\n",
    "2. Maintain a formal, academic, and objective tone.\n",
    "3. Each of the 4 versions must be linguistically different from the original and from each other.\n",
    "4. STRUCTURE CONSTRAINTS (MANDATORY):\n",
    "   - The source has two parts: Title and Abstract.\n",
    "   - You MUST output each paraphrase as a single string strictly in this format:\n",
    "     [Paraphrased Title]. [Paraphrased Abstract]\n",
    "   - Use exactly one period followed by a single space as the only separator between title and abstract.\n",
    "   - Do not add extra punctuation around the separator (no double periods, no colon).\n",
    "   - Keep the title as a concise noun phrase (no trailing punctuation other than the required separator).\n",
    "   - Do not move information between sections: title content stays in the title; abstract content stays in the abstract.\n",
    "   - Do not add leading or trailing quotation marks in the final strings.\n",
    "5. Format your response as a valid JSON object with a single key called \"paraphrased_versions\", which contains a list of the 4 paraphrased strings.\n",
    "\n",
    "Original Title:\n",
    "---\n",
    "{title}\n",
    "---\n",
    "\n",
    "Original Abstract:\n",
    "---\n",
    "{abstract}\n",
    "---\n",
    "\n",
    "JSON Output:\n",
    "\"\"\"\n",
    "\n",
    "# --- 3.1 Utilidad para separar Título y Abstract del campo 'text' (NUEVA) ---\n",
    "def split_title_abstract(txt: str):\n",
    "    \"\"\"\n",
    "    Separa el primer '. ' como límite entre Título y Abstract.\n",
    "    Si no encuentra '. ', intenta con el primer '.'.\n",
    "    Si aún así no hay '.', retorna ('', txt) como fallback.\n",
    "    \"\"\"\n",
    "    s = str(txt).strip()\n",
    "    if \". \" in s:\n",
    "        t, a = s.split(\". \", 1)\n",
    "        return t.strip(), a.strip()\n",
    "    idx = s.find(\".\")\n",
    "    if idx != -1:\n",
    "        return s[:idx].strip(), s[idx+1:].lstrip()\n",
    "    return \"\", s\n",
    "\n",
    "# --- 4. Bucle de Generación Guiado por el Plan ---\n",
    "new_data = []\n",
    "\n",
    "# (Opcional) PARA UNA PRUEBA RÁPIDA\n",
    "#plan_de_prueba = {k: v for i, (k, v) in enumerate(final_plan.items()) if i < 1}\n",
    "#plan_a_ejecutar = plan_de_prueba\n",
    "plan_a_ejecutar = final_plan  # Para la ejecución completa\n",
    "\n",
    "pbar = tqdm(total=sum(plan_a_ejecutar.values()), desc=\"🤖 Generando paráfrasis\")\n",
    "\n",
    "for combo, num_calls_for_combo in plan_a_ejecutar.items():\n",
    "    # Selecciona los artículos de la combinación actual según lo planeado\n",
    "    source_df = df_train[df_train['domain'] == combo].head(num_calls_for_combo)\n",
    "    \n",
    "    for index, row in source_df.iterrows():\n",
    "        original_text = row['text']\n",
    "        # --- (NUEVO) separar explícitamente Título y Abstract y pasarlos en el prompt ---\n",
    "        t_part, a_part = split_title_abstract(original_text)\n",
    "        prompt = prompt_template.format(title=t_part, abstract=a_part)\n",
    "        \n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            response_data = json.loads(response.text)\n",
    "            paraphrased_versions = response_data[\"paraphrased_versions\"]\n",
    "            \n",
    "            for version_text in paraphrased_versions:\n",
    "                if isinstance(version_text, str) and version_text.strip():\n",
    "                    # (Opcional) Validación ligera de estructura: debe contener '. ' al menos una vez\n",
    "                    if \". \" not in version_text:\n",
    "                        # No forzamos corrección automática para no corromper contenido,\n",
    "                        # solo registramos la anomalía si quieres monitorear.\n",
    "                        pass\n",
    "                    new_row = row.to_dict()\n",
    "                    new_row['text'] = version_text.strip()\n",
    "                    new_data.append(new_row)\n",
    "            \n",
    "            pbar.set_postfix_str(f\"Éxito: '{combo[:25]}...'\")\n",
    "        except Exception as e:\n",
    "            pbar.set_postfix_str(f\"Error en '{combo[:20]}...': {e}\")\n",
    "            time.sleep(5)\n",
    "            \n",
    "        time.sleep(1.5)  # Pausa cortés entre peticiones\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# --- 5. Creación y Guardado del Dataset Expandido ---\n",
    "if new_data:\n",
    "    augmented_df = pd.DataFrame(new_data)\n",
    "    \n",
    "    # Prepara el df original (sin la columna 'domain' auxiliar)\n",
    "    df_train_original = df_train.drop(columns=['domain'])\n",
    "    # Prepara el df aumentado (sin la columna 'domain' auxiliar)\n",
    "    if 'domain' in augmented_df.columns:\n",
    "        augmented_df = augmented_df.drop(columns=['domain'])\n",
    "    \n",
    "    # Combina el original con el nuevo\n",
    "    final_expanded_df = pd.concat([df_train_original, augmented_df], ignore_index=True)\n",
    "\n",
    "    print(f\"\\n¡Proceso completado! ✅\")\n",
    "    print(f\"Tamaño del dataset de entrenamiento original: {len(df_train_original)}\")\n",
    "    print(f\"Número de muestras nuevas generadas: {len(augmented_df)}\")\n",
    "    print(f\"Tamaño del dataset expandido: {len(final_expanded_df)}\")\n",
    "    \n",
    "    # Guardado\n",
    "    output_filename = \"data\\processed\\train_set_expanded.csv\"\n",
    "    final_expanded_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Nuevo dataset guardado como: '{output_filename}'\")\n",
    "else:\n",
    "    print(\"\\nNo se generaron nuevos datos. Revisa la barra de progreso por si hubo errores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa896f60",
   "metadata": {},
   "source": [
    "## generacion de datos de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5ea2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando la evaluación final en el conjunto de prueba ---\n",
      "Cargando modelo y tokenizador desde: models/scibert_uncased\n",
      "Cargando conjunto de prueba desde: database/test_set.csv\n",
      "Etiquetas detectadas automáticamente: ['cardiovascular', 'hepatorenal', 'neurological', 'oncological']\n",
      "Usando umbral por defecto para todas las etiquetas: 0.5\n",
      "Modelo movido al dispositivo: cpu\n",
      "Iniciando predicción en lotes de tamaño 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando Lotes: 100%|██████████| 12/12 [09:36<00:00, 48.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción en lotes completada.\n",
      "Construyendo el DataFrame de resultados...\n",
      "DataFrame de resultados construido con éxito. Muestra:\n",
      "                                                text  cardiovascular_true  \\\n",
      "0  An investigation of the pattern of kidney inju...                    0   \n",
      "1  Effect of fucoidan treatment on collagenase-in...                    0   \n",
      "2  Evaluation of the anticocaine monoclonal antib...                    0   \n",
      "3  renal stone markers in valvular heart disease....                    1   \n",
      "4  Evidence for a cholinergic role in haloperidol...                    0   \n",
      "\n",
      "   cardiovascular_prob  cardiovascular_pred  hepatorenal_true  \\\n",
      "0             0.013677                    0                 1   \n",
      "1             0.022620                    0                 0   \n",
      "2             0.012825                    0                 1   \n",
      "3             0.864140                    1                 1   \n",
      "4             0.015429                    0                 0   \n",
      "\n",
      "   hepatorenal_prob  hepatorenal_pred  neurological_true  neurological_prob  \\\n",
      "0          0.985933                 1                  0           0.278162   \n",
      "1          0.032011                 0                  1           0.990197   \n",
      "2          0.143327                 0                  1           0.986548   \n",
      "3          0.985373                 1                  0           0.013870   \n",
      "4          0.031571                 0                  1           0.987018   \n",
      "\n",
      "   neurological_pred  oncological_true  oncological_prob  oncological_pred  \n",
      "0                  0                 0          0.019288                 0  \n",
      "1                  1                 0          0.015091                 0  \n",
      "2                  1                 0          0.020537                 0  \n",
      "3                  0                 0          0.028457                 0  \n",
      "4                  1                 0          0.015303                 0  \n",
      "\n",
      "✅ ¡Éxito! Resultados guardados en: database\\test_predictions.csv\n",
      "Este archivo ahora contiene todo lo necesario para alimentar las visualizaciones del dashboard.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELDA FINAL: EVALUACIÓN EN TEST Y EXPORTACIÓN PARA DASHBOARD (VERSIÓN ESTÁNDAR)\n",
    "# ==============================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import json\n",
    "from tqdm.auto import tqdm # Para una barra de progreso visual\n",
    "\n",
    "print(\"--- Iniciando la evaluación final en el conjunto de prueba ---\")\n",
    "\n",
    "\n",
    "# --- 1. CONFIGURACIÓN DE RUTAS ---\n",
    "# Rutas relativas para que funcione en cualquier PC con la misma estructura de carpetas\n",
    "MODEL_DIR = \"models/scibert_uncased\"\n",
    "TEST_DATA_PATH = \"data\\processed\\test_set.csv\"\n",
    "OUTPUT_DIR = \"data\" # <-- CAMBIO: Simplificado para guardar en la carpeta principal de la base de datos\n",
    "OUTPUT_FILENAME = os.path.join(OUTPUT_DIR, \"test_predictions.csv\") # <-- CAMBIO: Nombre del archivo de salida\n",
    "\n",
    "\n",
    "# Asegúrate de que el directorio de salida exista\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- 2. CARGA DE ACTIVOS (MODELO, TOKENIZADOR, DATOS) ---\n",
    "print(f\"Cargando modelo y tokenizador desde: {MODEL_DIR}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "\n",
    "print(f\"Cargando conjunto de prueba desde: {TEST_DATA_PATH}\")\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "# --- CAMBIO: Detectar etiquetas automáticamente y usar umbral por defecto ---\n",
    "# Ya no cargamos el archivo 'best_thresholds.json'\n",
    "DEFAULT_THRESHOLD = 0.5\n",
    "LABEL_COLUMNS = [col for col in test_df.columns if col not in ['text', 'abstract', 'title', 'id']]\n",
    "\n",
    "print(f\"Etiquetas detectadas automáticamente: {LABEL_COLUMNS}\")\n",
    "print(f\"Usando umbral por defecto para todas las etiquetas: {DEFAULT_THRESHOLD}\")\n",
    "\n",
    "# Configuración del dispositivo (GPU si está disponible, si no CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"Modelo movido al dispositivo: {device}\")\n",
    "\n",
    "\n",
    "# --- 3. PREDICCIÓN EN LOTES (BATCH PREDICTION) ---\n",
    "# (Esta sección no necesita cambios)\n",
    "BATCH_SIZE = 32\n",
    "all_probabilities = []\n",
    "texts_to_predict = test_df['text'].tolist()\n",
    "\n",
    "print(f\"Iniciando predicción en lotes de tamaño {BATCH_SIZE}...\")\n",
    "\n",
    "for i in tqdm(range(0, len(texts_to_predict), BATCH_SIZE), desc=\"Procesando Lotes\"):\n",
    "    batch_texts = texts_to_predict[i:i + BATCH_SIZE]\n",
    "    \n",
    "    inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    probabilities = F.sigmoid(outputs.logits).cpu().numpy()\n",
    "    all_probabilities.append(probabilities)\n",
    "\n",
    "all_probabilities = np.vstack(all_probabilities)\n",
    "print(\"Predicción en lotes completada.\")\n",
    "\n",
    "\n",
    "# --- 4. CONSTRUCCIÓN DEL DATAFRAME FINAL DE RESULTADOS ---\n",
    "print(\"Construyendo el DataFrame de resultados...\")\n",
    "results_df = pd.DataFrame()\n",
    "results_df['text'] = test_df['text']\n",
    "\n",
    "for i, label in enumerate(LABEL_COLUMNS):\n",
    "    results_df[f'{label}_true'] = test_df[label]\n",
    "    results_df[f'{label}_prob'] = all_probabilities[:, i]\n",
    "    # <-- CAMBIO: Se aplica el umbral por defecto en lugar de los umbrales optimizados\n",
    "    results_df[f'{label}_pred'] = (all_probabilities[:, i] >= DEFAULT_THRESHOLD).astype(int)\n",
    "\n",
    "print(\"DataFrame de resultados construido con éxito. Muestra:\")\n",
    "print(results_df.head())\n",
    "\n",
    "\n",
    "# --- 5. GUARDAR EL ARCHIVO FINAL ---\n",
    "results_df.to_csv(OUTPUT_FILENAME, index=False)\n",
    "print(f\"\\n✅ ¡Éxito! Resultados guardados en: {OUTPUT_FILENAME}\")\n",
    "print(\"Este archivo ahora contiene todo lo necesario para alimentar las visualizaciones del dashboard.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
