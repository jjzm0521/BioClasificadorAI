{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a80fc3f",
   "metadata": {},
   "source": [
    "## preprocesamiento dataset\n",
    "\n",
    "\n",
    "Une el texto: Primero, toma el t√≠tulo y el resumen de cada investigaci√≥n y los junta en un solo p√°rrafo coherente.\n",
    "\n",
    "Estandariza las categor√≠as: Luego, revisa la columna que dice a qu√© especialidad m√©dica pertenece cada investigaci√≥n (ej. \"Oncolog√≠a\", \"cardiolog√≠a\", etc.) y la limpia para que todo est√© en min√∫sculas y sin espacios raros. As√≠, se asegura de que no haya confusiones.\n",
    "\n",
    "Traduce a n√∫meros: Esta es la parte clave. Crea nuevas columnas, una para cada especialidad m√©dica importante. Luego, para cada investigaci√≥n, marca con un 1 si pertenece a esa especialidad y con un 0 si no. Esto es como pasar una lista de asistencia, marcando \"presente\" o \"ausente\" para cada categor√≠a.\n",
    "\n",
    "Guarda el resultado final: Finalmente, se queda solo con lo importante (el p√°rrafo de texto y las nuevas columnas de 1s y 0s) y lo guarda todo en un archivo nuevo, ya limpio y listo para ser usado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ff312f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\raw\\\\challenge_data-18-ago.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Cargar dataset original\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mraw\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mchallenge_data-18-ago.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m;\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Funci√≥n robusta para unir t√≠tulo y abstract\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munir_texto\u001b[39m(title, abstract):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data\\\\raw\\\\challenge_data-18-ago.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar dataset original\n",
    "df = pd.read_csv(r\"data\\raw\\challenge_data-18-ago.csv\", sep=\";\")\n",
    "\n",
    "# Funci√≥n robusta para unir t√≠tulo y abstract\n",
    "def unir_texto(title, abstract):\n",
    "    title = str(title).strip()   # quitar espacios en los extremos\n",
    "    abstract = str(abstract).strip()\n",
    "    if title.endswith(\".\"):      # si ya tiene punto final\n",
    "        return title + \" \" + abstract\n",
    "    else:                        # si no lo tiene, se lo agregamos\n",
    "        return title + \". \" + abstract\n",
    "\n",
    "# Crear columna text\n",
    "df[\"text\"] = df.apply(lambda row: unir_texto(row[\"title\"], row[\"abstract\"]), axis=1)\n",
    "\n",
    "# Normalizamos categor√≠as\n",
    "df[\"group\"] = df[\"group\"].str.lower().str.strip()\n",
    "\n",
    "# Crear columnas multietiqueta\n",
    "for label in [\"cardiovascular\", \"hepatorenal\", \"neurological\", \"oncological\"]:\n",
    "    df[label] = df[\"group\"].apply(lambda x: 1 if label in x else 0)\n",
    "\n",
    "# Nos quedamos solo con lo necesario\n",
    "df_final = df[[\"text\", \"cardiovascular\", \"hepatorenal\", \"neurological\", \"oncological\"]]\n",
    "\n",
    "# Exportar\n",
    "df_final.to_csv(\"data\\processed\\dataset_preprocesado.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"‚úÖ Archivo procesado y guardado en data\\processed\\dataset_preprocesado.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ac3e8",
   "metadata": {},
   "source": [
    "## Separacion en entrenamiento, validacion, prueba\n",
    "En resumen, este c√≥digo es un planificador estrat√©gico para dividir los datos. Su objetivo es asegurarse de que, cuando separemos los datos para entrenar y probar la inteligencia artificial, la divisi√≥n sea justa e inteligente, especialmente con los casos m√°s raros.\n",
    "\n",
    "Esto es lo que hace paso a paso:\n",
    "\n",
    "Analiza las Combinaciones: Primero, lee el archivo de datos ya limpio. En lugar de mirar cada enfermedad por separado, se fija en las combinaciones de enfermedades que tiene cada investigaci√≥n. Por ejemplo, identifica si un estudio trata solo de \"c√°ncer\", o si trata de \"c√°ncer + neurolog√≠a\" al mismo tiempo. A cada combinaci√≥n le pone una etiqueta √∫nica.\n",
    "\n",
    "Cuenta los Grupos: Una vez que ha etiquetado todas las investigaciones, cuenta cu√°ntos ejemplos hay de cada combinaci√≥n. B√°sicamente, hace un inventario para saber qu√© tan comunes o raras son. Por ejemplo, podr√≠a descubrir que hay 500 estudios solo de \"c√°ncer\" pero solo 3 que son de \"c√°ncer + neurolog√≠a\".\n",
    "\n",
    "Crea un Plan de Reparto Inteligente (80/10/10): Esta es la parte m√°s importante. Sabiendo cu√°ntos ejemplos hay de cada tipo, dise√±a un plan para dividirlos en tres montones:\n",
    "\n",
    "Entrenamiento (80%): El mont√≥n m√°s grande, para que el modelo aprenda.\n",
    "Validaci√≥n (10%): Un peque√±o mont√≥n para hacer pruebas durante el entrenamiento.\n",
    "Prueba (10%): El mont√≥n final para ver qu√© tan bueno es el modelo.\n",
    "La clave es que lo hace de forma muy cuidadosa. Si una combinaci√≥n de enfermedades es muy rara (por ejemplo, solo hay 2 o 3 casos), se asegura de no \"romper\" ese grupito, poni√©ndolos juntos en el mont√≥n de entrenamiento para que el modelo al menos pueda aprender de ellos. Para las combinaciones m√°s comunes, s√≠ las reparte en los tres montones siguiendo la proporci√≥n 80/10/10.\n",
    "\n",
    "Al final, lo que te muestra en pantalla no son los datos divididos todav√≠a, sino el plan detallado de c√≥mo se van a dividir, garantizando que cada combinaci√≥n, por rara que sea, est√© representada de forma justa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ecb3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 1: Archivo 'dataset_preprocesado.csv' cargado exitosamente.\n",
      "Paso 1: Distribuci√≥n de combinaciones calculada.\n",
      "\n",
      "Paso 2: Plan de divisi√≥n 80/10/10 generado:\n",
      "                                                 domain  count  train_count  val_count  test_count\n",
      "0                                          neurological   1058          846        106         106\n",
      "1                                        cardiovascular    645          517         64          64\n",
      "2                                           hepatorenal    533          427         53          53\n",
      "3                           cardiovascular+neurological    308          246         31          31\n",
      "4                                           oncological    237          189         24          24\n",
      "5                              hepatorenal+neurological    202          162         20          20\n",
      "6                            cardiovascular+hepatorenal    190          152         19          19\n",
      "7                              neurological+oncological    143          115         14          14\n",
      "8                               hepatorenal+oncological     98           78         10          10\n",
      "9                            cardiovascular+oncological     70           56          7           7\n",
      "10              cardiovascular+hepatorenal+neurological     28           22          3           3\n",
      "11                 hepatorenal+neurological+oncological     26           20          3           3\n",
      "12              cardiovascular+neurological+oncological     13           11          1           1\n",
      "13               cardiovascular+hepatorenal+oncological      7            5          1           1\n",
      "14  cardiovascular+hepatorenal+neurological+oncological      7            5          1           1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- PASO 1: Leer el CSV y calcular la distribuci√≥n de combinaciones ---\n",
    "\n",
    "try:\n",
    "    df_raw = pd.read_csv('data/dataset_preprocesado.csv')\n",
    "    print(\"Paso 1: Archivo 'dataset_preprocesado.csv' cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: El archivo 'dataset_preprocesado.csv' no fue encontrado.\")\n",
    "    exit()\n",
    "\n",
    "# Columnas que representan las enfermedades\n",
    "disease_columns = ['cardiovascular', 'hepatorenal', 'neurological', 'oncological']\n",
    "\n",
    "# Funci√≥n para crear la cadena de combinaci√≥n para cada fila\n",
    "def create_domain_string(row):\n",
    "    present_diseases = [col for col in disease_columns if row[col] == 1]\n",
    "    if present_diseases:\n",
    "        return '+'.join(present_diseases)\n",
    "    return 'none' # Etiqueta para filas sin ninguna de estas enfermedades\n",
    "\n",
    "# Crear una nueva columna 'domain' con la combinaci√≥n de enfermedades\n",
    "df_raw['domain'] = df_raw.apply(create_domain_string, axis=1)\n",
    "\n",
    "# Calcular la distribuci√≥n (excluyendo las que no tienen ninguna enfermedad)\n",
    "distribution_counts = df_raw[df_raw['domain'] != 'none']['domain'].value_counts().reset_index()\n",
    "distribution_counts.columns = ['domain', 'count']\n",
    "print(\"Paso 1: Distribuci√≥n de combinaciones calculada.\\n\")\n",
    "\n",
    "\n",
    "# --- PASO 2: Crear el plan de divisi√≥n (80/10/10) ---\n",
    "\n",
    "def split_data_counts(row):\n",
    "    \"\"\"Calcula cu√°ntas muestras van a train/val/test para una fila de la tabla de distribuci√≥n.\"\"\"\n",
    "    count = row['count']\n",
    "    if count == 1:\n",
    "        return pd.Series([1, 0, 0], index=['train_count', 'val_count', 'test_count'])\n",
    "    elif count == 2:\n",
    "        return pd.Series([1, 1, 0], index=['train_count', 'val_count', 'test_count'])\n",
    "    elif count == 3:\n",
    "        return pd.Series([1, 1, 1], index=['train_count', 'val_count', 'test_count'])\n",
    "    else:\n",
    "        val_count = max(1, int(np.round(count * 0.1)))\n",
    "        test_count = max(1, int(np.round(count * 0.1)))\n",
    "        train_count = count - val_count - test_count\n",
    "        return pd.Series([train_count, val_count, test_count], index=['train_count', 'val_count', 'test_count'])\n",
    "\n",
    "# Aplicar la funci√≥n para obtener el plan de divisi√≥n\n",
    "split_plan = distribution_counts.copy()\n",
    "split_counts = split_plan.apply(split_data_counts, axis=1)\n",
    "split_plan = pd.concat([split_plan, split_counts], axis=1)\n",
    "\n",
    "print(\"Paso 2: Plan de divisi√≥n 80/10/10 generado:\")\n",
    "print(split_plan.to_string())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83002711",
   "metadata": {},
   "source": [
    "## ejecucion de la separacion del dataset\n",
    "\n",
    "En resumen, si el c√≥digo anterior era el \"planificador\", este c√≥digo es el \"ejecutor\". Toma el plan que se cre√≥ y lo lleva a cabo, repartiendo f√≠sicamente cada dato en el mont√≥n que le corresponde.\n",
    "\n",
    "As√≠ es como lo hace, paso a paso:\n",
    "\n",
    "Repartir los Datos, Grupo por Grupo:\n",
    "\n",
    "El c√≥digo revisa el plan, fila por fila. Cada fila corresponde a una combinaci√≥n de enfermedades (ej. \"solo c√°ncer\", o \"c√°ncer + neurolog√≠a\").\n",
    "Para cada combinaci√≥n, busca todos los estudios que pertenecen a ese grupo en la tabla de datos original.\n",
    "Luego, baraja aleatoriamente ese peque√±o grupo de estudios. Esto es como barajar una parte de la baraja para que el reparto sea justo.\n",
    "Finalmente, \"corta\" ese grupo barajado seg√∫n los n√∫meros del plan: los primeros van al mont√≥n de entrenamiento, los siguientes al de validaci√≥n, y los √∫ltimos al de prueba.\n",
    "Repite este proceso para todas y cada una de las combinaciones de enfermedades hasta que no queda ning√∫n dato sin asignar.\n",
    "Juntar y Mezclar los Montones Finales:\n",
    "\n",
    "Despu√©s del reparto, ahora tiene un mont√≥n de peque√±os \"sub-grupos\" para entrenamiento, validaci√≥n y prueba.\n",
    "Lo que hace a continuaci√≥n es juntar todos los pedacitos de \"entrenamiento\" en un √∫nico y gran archivo de entrenamiento. Hace lo mismo para los otros dos.\n",
    "Para terminar, vuelve a barajar cada uno de los tres montones grandes. Esto es muy importante para que el modelo de inteligencia artificial aprenda de forma desordenada y no, por ejemplo, viendo todos los casos de c√°ncer juntos.\n",
    "Guardar los Resultados:\n",
    "\n",
    "Una vez que los tres conjuntos de datos (entrenamiento, validaci√≥n y prueba) est√°n listos, limpios y bien mezclados, los guarda en tres archivos CSV separados.\n",
    "Hacer una √öltima Verificaci√≥n:\n",
    "\n",
    "Al final, simplemente hace una suma r√°pida. Cuenta cu√°ntos datos hab√≠a al principio y cu√°ntos hay en los tres nuevos archivos sumados, para asegurarse de que no se perdi√≥ ninguna investigaci√≥n en el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a84dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 3: Iniciando la divisi√≥n de datos por cada combinaci√≥n...\n",
      "Paso 3: Divisi√≥n completada.\n",
      "\n",
      "Paso 4: Archivos generados exitosamente:\n",
      "- train_set.csv\n",
      "- val_set.csv\n",
      "- test_set.csv\n",
      "\n",
      "--- Verificaci√≥n Final ---\n",
      "Total de muestras originales: 3565\n",
      "Muestras en train_set.csv: 2851\n",
      "Muestras en val_set.csv:   357\n",
      "Muestras en test_set.csv:  357\n",
      "Suma total:                3565\n",
      "¬°Proceso finalizado!\n"
     ]
    }
   ],
   "source": [
    "# --- PASO 3: Dividir el DataFrame original y crear los 3 conjuntos de datos ---\n",
    "\n",
    "# Listas para almacenar los dataframes de cada conjunto\n",
    "train_dfs, val_dfs, test_dfs = [], [], []\n",
    "\n",
    "print(\"Paso 3: Iniciando la divisi√≥n de datos por cada combinaci√≥n...\")\n",
    "\n",
    "# Iterar sobre el plan de divisi√≥n\n",
    "for _, row in split_plan.iterrows():\n",
    "    domain = row['domain']\n",
    "    train_num = row['train_count']\n",
    "    val_num = row['val_count']\n",
    "    \n",
    "    # Filtrar el dataframe original por la combinaci√≥n actual\n",
    "    domain_df = df_raw[df_raw['domain'] == domain]\n",
    "    \n",
    "    # Barajar aleatoriamente los datos de esta combinaci√≥n para asegurar una divisi√≥n imparcial\n",
    "    shuffled_domain_df = domain_df.sample(frac=1, random_state=42) # random_state para reproducibilidad\n",
    "    \n",
    "    # Cortar y asignar los datos a los conjuntos correspondientes\n",
    "    train_slice = shuffled_domain_df.iloc[:train_num]\n",
    "    val_slice = shuffled_domain_df.iloc[train_num:train_num + val_num]\n",
    "    test_slice = shuffled_domain_df.iloc[train_num + val_num:]\n",
    "    \n",
    "    # A√±adir las rebanadas a las listas\n",
    "    train_dfs.append(train_slice)\n",
    "    val_dfs.append(val_slice)\n",
    "    test_dfs.append(test_slice)\n",
    "\n",
    "# Combinar todas las rebanadas en tres DataFrames finales\n",
    "train_set = pd.concat(train_dfs)\n",
    "val_set = pd.concat(val_dfs)\n",
    "test_set = pd.concat(test_dfs)\n",
    "\n",
    "# Barajar los conjuntos finales para que las combinaciones no queden agrupadas\n",
    "train_set = train_set.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_set = val_set.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_set = test_set.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Eliminar la columna auxiliar 'domain' antes de guardar\n",
    "train_set = train_set.drop(columns=['domain'])\n",
    "val_set = val_set.drop(columns=['domain'])\n",
    "test_set = test_set.drop(columns=['domain'])\n",
    "\n",
    "print(\"Paso 3: Divisi√≥n completada.\\n\")\n",
    "\n",
    "\n",
    "# --- PASO 4: Guardar los archivos CSV ---\n",
    "\n",
    "train_set.to_csv('data\\processed\\train_set.csv', index=False)\n",
    "val_set.to_csv('data\\processed\\val_set.csv', index=False)\n",
    "test_set.to_csv('data\\processed\\test_set.csv', index=False)\n",
    "\n",
    "print(\"Paso 4: Archivos generados exitosamente:\")\n",
    "print(\"- train_set.csv\")\n",
    "print(\"- val_set.csv\")\n",
    "print(\"- test_set.csv\\n\")\n",
    "\n",
    "# --- Verificaci√≥n Final ---\n",
    "print(\"--- Verificaci√≥n Final ---\")\n",
    "print(f\"Total de muestras originales: {len(df_raw[df_raw['domain'] != 'none'])}\")\n",
    "print(f\"Muestras en train_set.csv: {len(train_set)}\")\n",
    "print(f\"Muestras en val_set.csv:   {len(val_set)}\")\n",
    "print(f\"Muestras en test_set.csv:  {len(test_set)}\")\n",
    "print(f\"Suma total:                {len(train_set) + len(val_set) + len(test_set)}\")\n",
    "print(\"¬°Proceso finalizado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c6902",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "En resumen, este c√≥digo es un \"estratega de datos\". Su trabajo es analizar el conjunto de datos de entrenamiento, encontrar d√≥nde hay desequilibrios (es decir, qu√© categor√≠as tienen muy pocos ejemplos) y crear un plan detallado para solucionar ese problema generando datos nuevos. La t√©cnica que planea usar es la \"par√°frasis\": tomar un art√≠culo existente y reescribirlo para crear una nueva versi√≥n.\n",
    "\n",
    "Aqu√≠ tienes el desglose de su estrategia:\n",
    "\n",
    "Hacer un Inventario: Primero, abre el archivo de datos de entrenamiento y, una vez m√°s, cuenta cu√°ntos art√≠culos hay para cada combinaci√≥n espec√≠fica de enfermedades. Este inventario es la base de toda su estrategia.\n",
    "\n",
    "Dise√±ar un Plan de \"Relleno\" en Tres Fases: El c√≥digo es muy inteligente y no trata a todos los art√≠culos por igual. Sigue un plan jer√°rquico para decidir qu√© art√≠culos duplicar:\n",
    "\n",
    "Fase 1: Los m√°s complejos y raros. Empieza con los art√≠culos que cubren 3 o 4 enfermedades a la vez. Sabe que estos son muy valiosos. Su estrategia es nivelar los grupos: si la combinaci√≥n m√°s com√∫n de 3 enfermedades tiene 20 ejemplos, planea crear par√°frasis de las otras combinaciones de 3 enfermedades hasta que todas lleguen a 20.\n",
    "\n",
    "Fase 2: Los de en medio. Luego, hace lo mismo con los art√≠culos que cubren 2 enfermedades. Busca la combinaci√≥n m√°s popular y planea \"rellenar\" las dem√°s para que alcancen ese mismo nivel.\n",
    "\n",
    "Fase 3: El Gran Balance Final. Despu√©s de nivelar los grupos m√°s complejos, mira el panorama general. Calcula el total de art√≠culos para cada enfermedad individual (por ejemplo, cu√°ntos estudios sobre \"c√°ncer\" hay en total, sin importar con qu√© est√© combinado). Identifica la enfermedad con m√°s ejemplos y establece ese n√∫mero como el objetivo final para todas las dem√°s. Luego, planea usar los art√≠culos m√°s simples (los de una sola enfermedad) como \"relleno\" para aumentar el conteo de las categor√≠as que se quedaron atr√°s, hasta que todas est√©n equilibradas.\n",
    "\n",
    "Presentar el Plan de Acci√≥n y los Resultados Esperados:\n",
    "\n",
    "Al final, el c√≥digo no ejecuta la creaci√≥n de datos. En su lugar, te presenta el plan completo y listo para ejecutar. Te dice exactamente cu√°ntos art√≠culos de cada combinaci√≥n espec√≠fica necesitas parafrasear.\n",
    "Tambi√©n te da una proyecci√≥n del \"antes y despu√©s\". Te muestra una tabla comparando la cantidad de datos que tienes ahora con la cantidad que tendr√°s si sigues el plan, demostrando que al final todo quedar√° perfectamente balanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d32071d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'train_set.csv' cargado correctamente.\n",
      "\n",
      "--- üìã PLAN DE BALANCEO H√çBRIDO FINAL (4 par√°frasis/llamada) üìã ---\n",
      "üîπ Parafrasear 163 art√≠culos de la combinaci√≥n: 'oncological'\n",
      "üîπ Parafrasear 104 art√≠culos de la combinaci√≥n: 'hepatorenal'\n",
      "üîπ Parafrasear 82 art√≠culos de la combinaci√≥n: 'cardiovascular'\n",
      "üîπ Parafrasear 48 art√≠culos de la combinaci√≥n: 'cardiovascular+oncological'\n",
      "üîπ Parafrasear 42 art√≠culos de la combinaci√≥n: 'hepatorenal+oncological'\n",
      "üîπ Parafrasear 33 art√≠culos de la combinaci√≥n: 'neurological+oncological'\n",
      "üîπ Parafrasear 24 art√≠culos de la combinaci√≥n: 'cardiovascular+hepatorenal'\n",
      "üîπ Parafrasear 21 art√≠culos de la combinaci√≥n: 'hepatorenal+neurological'\n",
      "üîπ Parafrasear 5 art√≠culos de la combinaci√≥n: 'cardiovascular+hepatorenal+neurological+oncological'\n",
      "üîπ Parafrasear 5 art√≠culos de la combinaci√≥n: 'cardiovascular+hepatorenal+oncological'\n",
      "üîπ Parafrasear 3 art√≠culos de la combinaci√≥n: 'cardiovascular+neurological+oncological'\n",
      "üîπ Parafrasear 1 art√≠culos de la combinaci√≥n: 'hepatorenal+neurological+oncological'\n",
      "\n",
      "-----------------------------------\n",
      "üìû Total de llamadas a la API necesarias: 531\n",
      "-----------------------------------\n",
      "\n",
      "--- üìà RESULTADOS PROYECTADOS (TOTALES DE CATEGOR√çA) üìà ---\n",
      "Categor√≠a       Inicial    Final     \n",
      "-----------------------------------\n",
      "Cardiovascular  1014       1682      \n",
      "Hepatorenal     871        1679      \n",
      "Neurological    1427       1679      \n",
      "Oncological     479        1679      \n",
      "\n",
      "--- üìä RESULTADOS PROYECTADOS (DISTRIBUCI√ìN DE COMBINACIONES) üìä ---\n",
      "Combinaci√≥n                                             Inicial    Final     \n",
      "---------------------------------------------------------------------------\n",
      "cardiovascular                                          517        845       \n",
      "hepatorenal                                             427        843       \n",
      "neurological                                            846        846       \n",
      "oncological                                             189        841       \n",
      "cardiovascular+hepatorenal                              152        248       \n",
      "cardiovascular+neurological                             246        246       \n",
      "cardiovascular+oncological                              56         248       \n",
      "hepatorenal+neurological                                162        246       \n",
      "hepatorenal+oncological                                 78         246       \n",
      "neurological+oncological                                115        247       \n",
      "cardiovascular+hepatorenal+neurological                 22         22        \n",
      "cardiovascular+hepatorenal+oncological                  5          25        \n",
      "cardiovascular+neurological+oncological                 11         23        \n",
      "hepatorenal+neurological+oncological                    20         24        \n",
      "cardiovascular+hepatorenal+neurological+oncological     5          25        \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# --- PASO 0: Leer el dataset de entrenamiento y calcular la distribuci√≥n ---\n",
    "\n",
    "try:\n",
    "    # Cargar el conjunto de entrenamiento generado previamente\n",
    "    df_train = pd.read_csv('database/train_set.csv')\n",
    "    print(\"Archivo 'train_set.csv' cargado correctamente.\\n\")\n",
    "\n",
    "    # Columnas que representan las enfermedades\n",
    "    disease_columns = ['cardiovascular', 'hepatorenal', 'neurological', 'oncological']\n",
    "\n",
    "    # Funci√≥n para crear la cadena de combinaci√≥n para cada fila\n",
    "    def create_domain_string(row):\n",
    "        present_diseases = [col for col in disease_columns if row[col] == 1]\n",
    "        if present_diseases:\n",
    "            return '+'.join(present_diseases)\n",
    "        return 'none'\n",
    "\n",
    "    # Crear la columna 'domain' y calcular la distribuci√≥n\n",
    "    df_train['domain'] = df_train.apply(create_domain_string, axis=1)\n",
    "    \n",
    "    # Convertir la serie de value_counts a un diccionario\n",
    "    combination_counts = df_train[df_train['domain'] != 'none']['domain'].value_counts().to_dict()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: No se encontr√≥ el archivo 'train_set.csv'.\")\n",
    "    print(\"Aseg√∫rate de haber ejecutado el script anterior para generarlo.\")\n",
    "    exit()\n",
    "\n",
    "# --- A PARTIR DE AQU√ç, ES EL SCRIPT DE OPTIMIZACI√ìN QUE PROPORCIONASTE ---\n",
    "\n",
    "# --- 1. DATOS INICIALES Y ESTRUCTURACI√ìN ---\n",
    "strata = {i: {} for i in range(1, 5)}\n",
    "for combo, count in combination_counts.items():\n",
    "    num_labels = combo.count('+') + 1\n",
    "    strata[num_labels][combo] = count\n",
    "\n",
    "# --- 2. PASO 1: REGLA FIJA PARA ART√çCULOS DE 4 ETIQUETAS ---\n",
    "paraphrase_plan = {combo: 0 for combo in combination_counts}\n",
    "\n",
    "if 4 in strata and strata[4]:\n",
    "    for combo, initial_count in strata[4].items():\n",
    "        paraphrase_plan[combo] = initial_count\n",
    "\n",
    "# --- 3. PASO 2: BALANCEAR ESTRATOS DE 3 Y 2 ETIQUETAS ---\n",
    "for i in [3, 2]:\n",
    "    stratum = strata.get(i, {})\n",
    "    if not stratum: continue\n",
    "    \n",
    "    target_count = max(stratum.values())\n",
    "    \n",
    "    for combo, initial_count in stratum.items():\n",
    "        if initial_count < target_count:\n",
    "            needed_articles = target_count - initial_count\n",
    "            calls_needed = math.ceil(needed_articles / 4)\n",
    "            paraphrase_plan[combo] += min(calls_needed, initial_count)\n",
    "\n",
    "# --- 4. C√ÅLCULO DE TOTALES INTERMEDIOS ---\n",
    "categories = [\"cardiovascular\", \"hepatorenal\", \"neurological\", \"oncological\"]\n",
    "intermediate_totals = {cat: 0 for cat in categories}\n",
    "for combo, count in combination_counts.items():\n",
    "    for cat in categories:\n",
    "        if cat in combo:\n",
    "            final_count = count + 4 * paraphrase_plan.get(combo, 0)\n",
    "            intermediate_totals[cat] += final_count\n",
    "\n",
    "# --- 5. PASO 3: USAR ART√çCULOS DE 1 ETIQUETA COMO RELLENO ---\n",
    "target_total = max(intermediate_totals.values())\n",
    "\n",
    "for cat in categories:\n",
    "    deficit = target_total - intermediate_totals[cat]\n",
    "    if deficit > 0:\n",
    "        calls_needed = math.ceil(deficit / 4)\n",
    "        single_label_combo = cat\n",
    "        initial_count = combination_counts.get(single_label_combo, 0)\n",
    "        # Asegurarnos que el combo de una etiqueta exista antes de a√±adir al plan\n",
    "        if single_label_combo in paraphrase_plan:\n",
    "             paraphrase_plan[single_label_combo] += min(calls_needed, initial_count)\n",
    "        else:\n",
    "             paraphrase_plan[single_label_combo] = min(calls_needed, initial_count)\n",
    "\n",
    "# --- 6. MOSTRAR RESULTADOS FINALES ---\n",
    "final_plan = {k: v for k, v in paraphrase_plan.items() if v > 0}\n",
    "total_api_calls = sum(final_plan.values())\n",
    "\n",
    "print(\"--- üìã PLAN DE BALANCEO H√çBRIDO FINAL (4 par√°frasis/llamada) üìã ---\")\n",
    "sorted_plan = sorted(final_plan.items(), key=lambda item: item[1], reverse=True)\n",
    "for combo, calls in sorted_plan:\n",
    "    print(f\"üîπ Parafrasear {calls} art√≠culos de la combinaci√≥n: '{combo}'\")\n",
    "\n",
    "print(\"\\n-----------------------------------\")\n",
    "print(f\"üìû Total de llamadas a la API necesarias: {total_api_calls}\")\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "\n",
    "print(\"\\n--- üìà RESULTADOS PROYECTADOS (TOTALES DE CATEGOR√çA) üìà ---\")\n",
    "final_totals = {cat: 0 for cat in categories}\n",
    "for combo, count in combination_counts.items():\n",
    "    for cat in categories:\n",
    "        if cat in combo:\n",
    "            final_totals[cat] += count + 4 * paraphrase_plan.get(combo, 0)\n",
    "\n",
    "print(f\"{'Categor√≠a':<15} {'Inicial':<10} {'Final':<10}\")\n",
    "print(\"-\" * 35)\n",
    "for cat in categories:\n",
    "    initial_total = sum(c for co, c in combination_counts.items() if cat in co)\n",
    "    print(f\"{cat.capitalize():<15} {initial_total:<10} {final_totals[cat]:<10}\")\n",
    "\n",
    "print(\"\\n--- üìä RESULTADOS PROYECTADOS (DISTRIBUCI√ìN DE COMBINACIONES) üìä ---\")\n",
    "print(f\"{'Combinaci√≥n':<55} {'Inicial':<10} {'Final':<10}\")\n",
    "print(\"-\" * 75)\n",
    "# Asegurarnos de mostrar todas las combinaciones originales\n",
    "sorted_combos = sorted(combination_counts.keys(), key=lambda x: (x.count('+'), x))\n",
    "for combo in sorted_combos:\n",
    "    initial = combination_counts.get(combo, 0)\n",
    "    final = initial + 4 * paraphrase_plan.get(combo, 0)\n",
    "    print(f\"{combo:<55} {initial:<10} {final:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1c4059",
   "metadata": {},
   "source": [
    "## Generacion de datos con Gemini \n",
    "\n",
    "En resumen, si el c√≥digo anterior era el \"estratega\", este es el \"operario de la f√°brica\". Su √∫nica misi√≥n es ejecutar el plan de creaci√≥n de datos, utilizando una potente inteligencia artificial (Gemini de Google) para hacer el trabajo pesado.\n",
    "\n",
    "Aqu√≠ est√° el proceso explicado de manera sencilla:\n",
    "\n",
    "Preparaci√≥n y Verificaci√≥n: Antes de encender las m√°quinas, el c√≥digo hace una revisi√≥n de seguridad.\n",
    "\n",
    "Primero, se asegura de que el \"plan\" creado en la celda anterior est√© disponible. Si no lo encuentra, detiene todo.\n",
    "Luego, busca la \"llave\" para acceder a la IA de Google (la API Key). Sin esta llave, no puede funcionar.\n",
    "Redactar las Instrucciones para la IA: Esta es una parte fundamental. El c√≥digo crea una plantilla de instrucciones muy clara y estricta para la inteligencia artificial. En esencia, le dice:\n",
    "\n",
    "\"Act√∫a como un experto editor de textos m√©dicos.\"\n",
    "\"Te dar√© un t√≠tulo y un resumen. Tu trabajo es reescribirlo 4 veces de formas diferentes.\"\n",
    "Reglas Cruciales: \"No te inventes nada, no cambies los datos, mant√©n un tono profesional y, lo m√°s importante, entr√©game cada versi√≥n en este formato exacto: T√≠tulo reescrito. Resumen reescrito.\"\n",
    "\"Dame el resultado final empaquetado en un formato espec√≠fico (JSON) para que yo lo pueda entender f√°cilmente.\"\n",
    "Poner en Marcha la Producci√≥n:\n",
    "\n",
    "El c√≥digo se convierte en un \"jefe de producci√≥n\". Revisa el plan y, para cada combinaci√≥n de enfermedades, toma la cantidad exacta de art√≠culos que necesita parafrasear.\n",
    "Va uno por uno. Coge un art√≠culo, lo mete en la plantilla de instrucciones y se lo env√≠a a la IA de Gemini.\n",
    "Mientras la IA trabaja, muestra una barra de progreso para que puedas ver en tiempo real c√≥mo avanza la creaci√≥n de los nuevos datos.\n",
    "Cuando la IA devuelve las 4 nuevas versiones del texto, el c√≥digo las recoge y las guarda en una lista temporal de \"datos nuevos\".\n",
    "Por cortes√≠a, hace una peque√±a pausa entre cada petici√≥n para no sobrecargar el servicio de la IA.\n",
    "Ensamblaje Final y Almacenamiento:\n",
    "\n",
    "Una vez que ha pasado por todo el plan y ha generado cientos de nuevos art√≠culos, toma la lista de \"datos nuevos\".\n",
    "La combina con el conjunto de datos de entrenamiento original.\n",
    "El resultado es un nuevo archivo de entrenamiento mucho m√°s grande y balanceado.\n",
    "Finalmente, guarda este archivo mejorado con el nombre train_set_expanded.csv, dej√°ndolo listo para el paso final: entrenar un modelo mucho m√°s robusto y preciso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb012c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Variables 'final_plan' y 'df_train' encontradas de la celda anterior.\n",
      "Se ejecutar√°n 531 llamadas a la API seg√∫n el plan.\n",
      "üîë API Key cargada desde la variable de entorno.\n",
      "ü§ñ Modelo seleccionado: models/gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ü§ñ Generando par√°frasis: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 531/531 [2:17:43<00:00, 15.56s/it, √âxito: 'cardiovascular+hepatorena...']                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "¬°Proceso completado! ‚úÖ\n",
      "Tama√±o del dataset de entrenamiento original: 2851\n",
      "N√∫mero de muestras nuevas generadas: 2120\n",
      "Tama√±o del dataset expandido: 4971\n",
      "Nuevo dataset guardado como: 'database/train_set_expanded.csv'\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SCRIPT DE EJECUCI√ìN DEL PLAN DE BALANCEO\n",
    "# (Esta celda asume que 'final_plan' y 'df_train' existen de la celda anterior)\n",
    "# ==============================================================================\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv \n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "# --- 1. Verificaci√≥n de Variables Previas ---\n",
    "try:\n",
    "    if 'final_plan' in locals() and 'df_train' in locals():\n",
    "        print(\"‚úÖ Variables 'final_plan' y 'df_train' encontradas de la celda anterior.\")\n",
    "        total_calls = sum(final_plan.values())\n",
    "        print(f\"Se ejecutar√°n {total_calls} llamadas a la API seg√∫n el plan.\")\n",
    "    else:\n",
    "        raise NameError\n",
    "except NameError:\n",
    "    print(\"‚ùå Error: Las variables 'final_plan' y 'df_train' no se encontraron.\")\n",
    "    print(\"Aseg√∫rate de ejecutar la celda anterior que calcula el plan de optimizaci√≥n.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Configuraci√≥n de la API ---\n",
    "load_dotenv() \n",
    "try:\n",
    "    API_KEY = os.environ.get('GEMINI_API_KEY')\n",
    "    if API_KEY is None: raise ValueError\n",
    "    print(\"üîë API Key cargada desde la variable de entorno.\")\n",
    "except (ValueError, KeyError):\n",
    "    API_KEY = \"PEGA_TU_API_KEY_DE_GEMINI_AQUI\" # üö® REEMPLAZA ESTO\n",
    "    print(\"‚ö†Ô∏è API Key no encontrada. Usando la del script. ¬°No olvides reemplazarla!\")\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-2.5-flash',\n",
    "    generation_config={\"response_mime_type\": \"application/json\"}\n",
    ")\n",
    "print(f\"ü§ñ Modelo seleccionado: {model.model_name}\")\n",
    "\n",
    "# --- 3. Definici√≥n del Prompt (MODIFICADO) ---\n",
    "prompt_template = \"\"\"\n",
    "Act as an expert medical and scientific text editor.\n",
    "Your task is to generate 4 distinct paraphrased versions of the provided research article title and abstract.\n",
    "\n",
    "Crucial Instructions:\n",
    "1. Preserve the factual meaning completely. Do not alter, invent, or omit any key medical entities, numerical results, or conclusions.\n",
    "2. Maintain a formal, academic, and objective tone.\n",
    "3. Each of the 4 versions must be linguistically different from the original and from each other.\n",
    "4. STRUCTURE CONSTRAINTS (MANDATORY):\n",
    "   - The source has two parts: Title and Abstract.\n",
    "   - You MUST output each paraphrase as a single string strictly in this format:\n",
    "     [Paraphrased Title]. [Paraphrased Abstract]\n",
    "   - Use exactly one period followed by a single space as the only separator between title and abstract.\n",
    "   - Do not add extra punctuation around the separator (no double periods, no colon).\n",
    "   - Keep the title as a concise noun phrase (no trailing punctuation other than the required separator).\n",
    "   - Do not move information between sections: title content stays in the title; abstract content stays in the abstract.\n",
    "   - Do not add leading or trailing quotation marks in the final strings.\n",
    "5. Format your response as a valid JSON object with a single key called \"paraphrased_versions\", which contains a list of the 4 paraphrased strings.\n",
    "\n",
    "Original Title:\n",
    "---\n",
    "{title}\n",
    "---\n",
    "\n",
    "Original Abstract:\n",
    "---\n",
    "{abstract}\n",
    "---\n",
    "\n",
    "JSON Output:\n",
    "\"\"\"\n",
    "\n",
    "# --- 3.1 Utilidad para separar T√≠tulo y Abstract del campo 'text' (NUEVA) ---\n",
    "def split_title_abstract(txt: str):\n",
    "    \"\"\"\n",
    "    Separa el primer '. ' como l√≠mite entre T√≠tulo y Abstract.\n",
    "    Si no encuentra '. ', intenta con el primer '.'.\n",
    "    Si a√∫n as√≠ no hay '.', retorna ('', txt) como fallback.\n",
    "    \"\"\"\n",
    "    s = str(txt).strip()\n",
    "    if \". \" in s:\n",
    "        t, a = s.split(\". \", 1)\n",
    "        return t.strip(), a.strip()\n",
    "    idx = s.find(\".\")\n",
    "    if idx != -1:\n",
    "        return s[:idx].strip(), s[idx+1:].lstrip()\n",
    "    return \"\", s\n",
    "\n",
    "# --- 4. Bucle de Generaci√≥n Guiado por el Plan ---\n",
    "new_data = []\n",
    "\n",
    "# (Opcional) PARA UNA PRUEBA R√ÅPIDA\n",
    "#plan_de_prueba = {k: v for i, (k, v) in enumerate(final_plan.items()) if i < 1}\n",
    "#plan_a_ejecutar = plan_de_prueba\n",
    "plan_a_ejecutar = final_plan  # Para la ejecuci√≥n completa\n",
    "\n",
    "pbar = tqdm(total=sum(plan_a_ejecutar.values()), desc=\"ü§ñ Generando par√°frasis\")\n",
    "\n",
    "for combo, num_calls_for_combo in plan_a_ejecutar.items():\n",
    "    # Selecciona los art√≠culos de la combinaci√≥n actual seg√∫n lo planeado\n",
    "    source_df = df_train[df_train['domain'] == combo].head(num_calls_for_combo)\n",
    "    \n",
    "    for index, row in source_df.iterrows():\n",
    "        original_text = row['text']\n",
    "        # --- (NUEVO) separar expl√≠citamente T√≠tulo y Abstract y pasarlos en el prompt ---\n",
    "        t_part, a_part = split_title_abstract(original_text)\n",
    "        prompt = prompt_template.format(title=t_part, abstract=a_part)\n",
    "        \n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            response_data = json.loads(response.text)\n",
    "            paraphrased_versions = response_data[\"paraphrased_versions\"]\n",
    "            \n",
    "            for version_text in paraphrased_versions:\n",
    "                if isinstance(version_text, str) and version_text.strip():\n",
    "                    # (Opcional) Validaci√≥n ligera de estructura: debe contener '. ' al menos una vez\n",
    "                    if \". \" not in version_text:\n",
    "                        # No forzamos correcci√≥n autom√°tica para no corromper contenido,\n",
    "                        # solo registramos la anomal√≠a si quieres monitorear.\n",
    "                        pass\n",
    "                    new_row = row.to_dict()\n",
    "                    new_row['text'] = version_text.strip()\n",
    "                    new_data.append(new_row)\n",
    "            \n",
    "            pbar.set_postfix_str(f\"√âxito: '{combo[:25]}...'\")\n",
    "        except Exception as e:\n",
    "            pbar.set_postfix_str(f\"Error en '{combo[:20]}...': {e}\")\n",
    "            time.sleep(5)\n",
    "            \n",
    "        time.sleep(1.5)  # Pausa cort√©s entre peticiones\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# --- 5. Creaci√≥n y Guardado del Dataset Expandido ---\n",
    "if new_data:\n",
    "    augmented_df = pd.DataFrame(new_data)\n",
    "    \n",
    "    # Prepara el df original (sin la columna 'domain' auxiliar)\n",
    "    df_train_original = df_train.drop(columns=['domain'])\n",
    "    # Prepara el df aumentado (sin la columna 'domain' auxiliar)\n",
    "    if 'domain' in augmented_df.columns:\n",
    "        augmented_df = augmented_df.drop(columns=['domain'])\n",
    "    \n",
    "    # Combina el original con el nuevo\n",
    "    final_expanded_df = pd.concat([df_train_original, augmented_df], ignore_index=True)\n",
    "\n",
    "    print(f\"\\n¬°Proceso completado! ‚úÖ\")\n",
    "    print(f\"Tama√±o del dataset de entrenamiento original: {len(df_train_original)}\")\n",
    "    print(f\"N√∫mero de muestras nuevas generadas: {len(augmented_df)}\")\n",
    "    print(f\"Tama√±o del dataset expandido: {len(final_expanded_df)}\")\n",
    "    \n",
    "    # Guardado\n",
    "    output_filename = \"data\\processed\\train_set_expanded.csv\"\n",
    "    final_expanded_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Nuevo dataset guardado como: '{output_filename}'\")\n",
    "else:\n",
    "    print(\"\\nNo se generaron nuevos datos. Revisa la barra de progreso por si hubo errores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa896f60",
   "metadata": {},
   "source": [
    "## generacion de datos de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5ea2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando la evaluaci√≥n final en el conjunto de prueba ---\n",
      "Cargando modelo y tokenizador desde: models/scibert_uncased\n",
      "Cargando conjunto de prueba desde: database/test_set.csv\n",
      "Etiquetas detectadas autom√°ticamente: ['cardiovascular', 'hepatorenal', 'neurological', 'oncological']\n",
      "Usando umbral por defecto para todas las etiquetas: 0.5\n",
      "Modelo movido al dispositivo: cpu\n",
      "Iniciando predicci√≥n en lotes de tama√±o 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando Lotes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [09:36<00:00, 48.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicci√≥n en lotes completada.\n",
      "Construyendo el DataFrame de resultados...\n",
      "DataFrame de resultados construido con √©xito. Muestra:\n",
      "                                                text  cardiovascular_true  \\\n",
      "0  An investigation of the pattern of kidney inju...                    0   \n",
      "1  Effect of fucoidan treatment on collagenase-in...                    0   \n",
      "2  Evaluation of the anticocaine monoclonal antib...                    0   \n",
      "3  renal stone markers in valvular heart disease....                    1   \n",
      "4  Evidence for a cholinergic role in haloperidol...                    0   \n",
      "\n",
      "   cardiovascular_prob  cardiovascular_pred  hepatorenal_true  \\\n",
      "0             0.013677                    0                 1   \n",
      "1             0.022620                    0                 0   \n",
      "2             0.012825                    0                 1   \n",
      "3             0.864140                    1                 1   \n",
      "4             0.015429                    0                 0   \n",
      "\n",
      "   hepatorenal_prob  hepatorenal_pred  neurological_true  neurological_prob  \\\n",
      "0          0.985933                 1                  0           0.278162   \n",
      "1          0.032011                 0                  1           0.990197   \n",
      "2          0.143327                 0                  1           0.986548   \n",
      "3          0.985373                 1                  0           0.013870   \n",
      "4          0.031571                 0                  1           0.987018   \n",
      "\n",
      "   neurological_pred  oncological_true  oncological_prob  oncological_pred  \n",
      "0                  0                 0          0.019288                 0  \n",
      "1                  1                 0          0.015091                 0  \n",
      "2                  1                 0          0.020537                 0  \n",
      "3                  0                 0          0.028457                 0  \n",
      "4                  1                 0          0.015303                 0  \n",
      "\n",
      "‚úÖ ¬°√âxito! Resultados guardados en: database\\test_predictions.csv\n",
      "Este archivo ahora contiene todo lo necesario para alimentar las visualizaciones del dashboard.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELDA FINAL: EVALUACI√ìN EN TEST Y EXPORTACI√ìN PARA DASHBOARD (VERSI√ìN EST√ÅNDAR)\n",
    "# ==============================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import json\n",
    "from tqdm.auto import tqdm # Para una barra de progreso visual\n",
    "\n",
    "print(\"--- Iniciando la evaluaci√≥n final en el conjunto de prueba ---\")\n",
    "\n",
    "\n",
    "# --- 1. CONFIGURACI√ìN DE RUTAS ---\n",
    "# Rutas relativas para que funcione en cualquier PC con la misma estructura de carpetas\n",
    "MODEL_DIR = \"models/scibert_uncased\"\n",
    "TEST_DATA_PATH = \"data\\processed\\test_set.csv\"\n",
    "OUTPUT_DIR = \"data\" # <-- CAMBIO: Simplificado para guardar en la carpeta principal de la base de datos\n",
    "OUTPUT_FILENAME = os.path.join(OUTPUT_DIR, \"test_predictions.csv\") # <-- CAMBIO: Nombre del archivo de salida\n",
    "\n",
    "\n",
    "# Aseg√∫rate de que el directorio de salida exista\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- 2. CARGA DE ACTIVOS (MODELO, TOKENIZADOR, DATOS) ---\n",
    "print(f\"Cargando modelo y tokenizador desde: {MODEL_DIR}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "\n",
    "print(f\"Cargando conjunto de prueba desde: {TEST_DATA_PATH}\")\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "# --- CAMBIO: Detectar etiquetas autom√°ticamente y usar umbral por defecto ---\n",
    "# Ya no cargamos el archivo 'best_thresholds.json'\n",
    "DEFAULT_THRESHOLD = 0.5\n",
    "LABEL_COLUMNS = [col for col in test_df.columns if col not in ['text', 'abstract', 'title', 'id']]\n",
    "\n",
    "print(f\"Etiquetas detectadas autom√°ticamente: {LABEL_COLUMNS}\")\n",
    "print(f\"Usando umbral por defecto para todas las etiquetas: {DEFAULT_THRESHOLD}\")\n",
    "\n",
    "# Configuraci√≥n del dispositivo (GPU si est√° disponible, si no CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"Modelo movido al dispositivo: {device}\")\n",
    "\n",
    "\n",
    "# --- 3. PREDICCI√ìN EN LOTES (BATCH PREDICTION) ---\n",
    "# (Esta secci√≥n no necesita cambios)\n",
    "BATCH_SIZE = 32\n",
    "all_probabilities = []\n",
    "texts_to_predict = test_df['text'].tolist()\n",
    "\n",
    "print(f\"Iniciando predicci√≥n en lotes de tama√±o {BATCH_SIZE}...\")\n",
    "\n",
    "for i in tqdm(range(0, len(texts_to_predict), BATCH_SIZE), desc=\"Procesando Lotes\"):\n",
    "    batch_texts = texts_to_predict[i:i + BATCH_SIZE]\n",
    "    \n",
    "    inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    probabilities = F.sigmoid(outputs.logits).cpu().numpy()\n",
    "    all_probabilities.append(probabilities)\n",
    "\n",
    "all_probabilities = np.vstack(all_probabilities)\n",
    "print(\"Predicci√≥n en lotes completada.\")\n",
    "\n",
    "\n",
    "# --- 4. CONSTRUCCI√ìN DEL DATAFRAME FINAL DE RESULTADOS ---\n",
    "print(\"Construyendo el DataFrame de resultados...\")\n",
    "results_df = pd.DataFrame()\n",
    "results_df['text'] = test_df['text']\n",
    "\n",
    "for i, label in enumerate(LABEL_COLUMNS):\n",
    "    results_df[f'{label}_true'] = test_df[label]\n",
    "    results_df[f'{label}_prob'] = all_probabilities[:, i]\n",
    "    # <-- CAMBIO: Se aplica el umbral por defecto en lugar de los umbrales optimizados\n",
    "    results_df[f'{label}_pred'] = (all_probabilities[:, i] >= DEFAULT_THRESHOLD).astype(int)\n",
    "\n",
    "print(\"DataFrame de resultados construido con √©xito. Muestra:\")\n",
    "print(results_df.head())\n",
    "\n",
    "\n",
    "# --- 5. GUARDAR EL ARCHIVO FINAL ---\n",
    "results_df.to_csv(OUTPUT_FILENAME, index=False)\n",
    "print(f\"\\n‚úÖ ¬°√âxito! Resultados guardados en: {OUTPUT_FILENAME}\")\n",
    "print(\"Este archivo ahora contiene todo lo necesario para alimentar las visualizaciones del dashboard.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
