{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datsets\n",
    "\n",
    "Este c√≥digo finaliza la preparaci√≥n de los datos antes de entrenar el modelo. Sus tareas principales son:\n",
    "\n",
    "Instalar librer√≠as: Asegura que todas las herramientas de software necesarias est√©n listas.\n",
    "\n",
    "Cargar datos: Importa los conjuntos de entrenamiento, validaci√≥n y prueba.\n",
    "\n",
    "Unificar etiquetas: Consolida las m√∫ltiples columnas de categor√≠as en una sola lista de etiquetas por cada art√≠culo.\n",
    "\n",
    "Formatear para IA: Convierte los datos al formato especializado que el modelo requiere para procesarlos eficientemente.\n",
    "\n",
    "Verificar: Confirma que todos los registros se hayan cargado correctamente en cada conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-08-25T05:47:05.466Z",
     "iopub.execute_input": "2025-08-25T05:46:58.562935Z",
     "iopub.status.busy": "2025-08-25T05:46:58.562641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 1: CARGA DE DATASETS (TRAIN / VAL / TEST) DESDE CSV\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"Instalando librer√≠as necesarias...\")\n",
    "# !pip install --upgrade transformers datasets scikit-learn -q\n",
    "print(\"¬°Instalaci√≥n completa!\")\n",
    "\n",
    "# Rutas a tus CSV ya separados\n",
    "TRAIN_PATH = \"/kaggle/input/split-dataset/train_set_expanded.csv\"\n",
    "VAL_PATH   = \"/kaggle/input/split-dataset/val_set.csv\"\n",
    "TEST_PATH  = \"/kaggle/input/split-dataset/test_set.csv\"\n",
    "\n",
    "TEXT_COLUMN = \"text\"\n",
    "LABEL_COLUMNS = [\"cardiovascular\", \"hepatorenal\", \"neurological\", \"oncological\"]\n",
    "\n",
    "def load_split(path):\n",
    "    df = pd.read_csv(path)\n",
    "    # Asegura tipos 0/1 en las etiquetas\n",
    "    for c in LABEL_COLUMNS:\n",
    "        df[c] = df[c].astype(int)\n",
    "    df[\"labels\"] = df[LABEL_COLUMNS].values.tolist()\n",
    "    return df[[TEXT_COLUMN, \"labels\"]].copy()\n",
    "\n",
    "try:\n",
    "    train_df = load_split(TRAIN_PATH)\n",
    "    val_df   = load_split(VAL_PATH)\n",
    "    test_df  = load_split(TEST_PATH)\n",
    "    print(\"‚úÖ Datasets cargados correctamente.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå No se encontr√≥ un archivo: {e}\")\n",
    "    raise\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset   = Dataset.from_pandas(val_df)\n",
    "test_dataset  = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(f\"Tama√±o train: {len(train_dataset)}\")\n",
    "print(f\"Tama√±o val:   {len(val_dataset)}\")\n",
    "print(f\"Tama√±o test:  {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacion\n",
    "\n",
    "Este script prepara un modelo SciBERT para una tarea de clasificaci√≥n multietiqueta.\n",
    "\n",
    "Primero, tokeniza los datasets de texto, convirti√©ndolos en tensores num√©ricos de longitud fija para PyTorch.\n",
    "\n",
    "Luego, instancia el modelo SciBERT y lo configura expl√≠citamente para multi_label_classification, lo que ajusta su arquitectura para predecir m√∫ltiples categor√≠as simult√°neamente.\n",
    "\n",
    "Finalmente, define una funci√≥n para evaluar el rendimiento del modelo utilizando m√©tricas clave como F1-score y ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 2: TOKENIZACI√ìN, MODELO (SCIBERT UNCASED) Y M√âTRICAS\n",
    "# ==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# Nombre del modelo (SciBERT uncased)\n",
    "MODEL_NAME = \"allenai/scibert_scivocab_uncased\"\n",
    "\n",
    "# Se asume que LABEL_COLUMNS, train_dataset, val_dataset, test_dataset\n",
    "# ya fueron creados en la Celda 1.\n",
    "id2label = {i: l for i, l in enumerate(LABEL_COLUMNS)}\n",
    "label2id = {l: i for i, l in enumerate(LABEL_COLUMNS)}\n",
    "\n",
    "# Tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    enc = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    # Etiquetas como float32 para BCEWithLogits\n",
    "    enc[\"labels\"] = [np.array(x, dtype=np.float32) for x in batch[\"labels\"]]\n",
    "    return enc\n",
    "\n",
    "# Tokenizaci√≥n de los splits\n",
    "train_enc = train_dataset.map(tokenize_batch, batched=True, remove_columns=train_dataset.column_names)\n",
    "val_enc   = val_dataset.map(tokenize_batch,   batched=True, remove_columns=val_dataset.column_names)\n",
    "test_enc  = test_dataset.map(tokenize_batch,  batched=True, remove_columns=test_dataset.column_names)\n",
    "\n",
    "# Formato PyTorch\n",
    "cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_enc = train_enc.with_format(\"torch\", columns=cols)\n",
    "val_enc   = val_enc.with_format(\"torch\",   columns=cols)\n",
    "test_enc  = test_enc.with_format(\"torch\",  columns=cols)\n",
    "\n",
    "# Modelo de clasificaci√≥n multilabel\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(LABEL_COLUMNS),\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# M√©tricas (incluye F1 ponderado)\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = 1 / (1 + np.exp(-logits))          # sigmoid\n",
    "    preds = (probs >= 0.5).astype(int)         # umbral base 0.5\n",
    "\n",
    "    f1_micro = f1_score(labels, preds, average=\"micro\", zero_division=0)\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    f1_weighted = f1_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    try:\n",
    "        auc_macro = roc_auc_score(labels, probs, average=\"macro\")\n",
    "    except ValueError:\n",
    "        auc_macro = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"roc_auc_macro\": auc_macro\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entrenamiento y  selecci√≥n de mejores hiperpar√°metros\n",
    "\n",
    "Este script ejecuta un proceso avanzado y automatizado de optimizaci√≥n de hiperpar√°metros (HPO) utilizando la librer√≠a Optuna para encontrar la configuraci√≥n de entrenamiento √≥ptima para el modelo SciBERT. El proceso no solo busca los mejores hiperpar√°metros, sino que tambi√©n implementa t√©cnicas sofisticadas para manejar el desequilibrio de clases y optimizar los umbrales de decisi√≥n. Finalmente, eval√∫a rigurosamente el modelo campe√≥n en el conjunto de datos de prueba y lo empaqueta para su uso futuro.\n",
    "\n",
    "Desglose Funcional Detallado\n",
    "Optimizaci√≥n de Hiperpar√°metros (HPO) con Optuna:\n",
    "\n",
    "El n√∫cleo del script es un bucle de optimizaci√≥n (study.optimize) que prueba sistem√°ticamente m√∫ltiples combinaciones de hiperpar√°metros clave (tasa de aprendizaje, tama√±o de lote, decaimiento de peso, etc.) para encontrar la que maximiza el rendimiento.\n",
    "El objetivo de cada \"trial\" (prueba) es maximizar la m√©trica f1_weighted en el conjunto de validaci√≥n, que es una m√©trica robusta para problemas con desequilibrio de clases.\n",
    "Manejo del Desequilibrio de Clases:\n",
    "\n",
    "Se implementa una funci√≥n de p√©rdida personalizada (compute_loss_with_pos_weight). Antes de entrenar, se calcula la frecuencia de cada etiqueta en el dataset de entrenamiento.\n",
    "La funci√≥n de p√©rdida (BCEWithLogitsLoss) utiliza estos c√°lculos para asignar un mayor peso a las clases minoritarias, forzando al modelo a prestarles m√°s atenci√≥n y evitando que se centre √∫nicamente en las clases m√°s comunes.\n",
    "Ajuste de Umbrales de Decisi√≥n por Clase (tune_thresholds_per_class):\n",
    "\n",
    "En la clasificaci√≥n multietiqueta, un umbral de decisi√≥n est√°ndar de 0.5 no suele ser √≥ptimo. Esta funci√≥n clave se ejecuta despu√©s de cada entrenamiento de un trial.\n",
    "Para cada una de las cuatro etiquetas, busca iterativamente el umbral de probabilidad (entre 0.05 y 0.95) que maximiza el F1-score individual para esa clase en el conjunto de validaci√≥n.\n",
    "El rendimiento final de un trial se mide despu√©s de aplicar estos umbrales optimizados, proporcionando una evaluaci√≥n mucho m√°s precisa del verdadero potencial del modelo.\n",
    "Gesti√≥n Eficiente de Recursos:\n",
    "\n",
    "Early Stopping: Se detienen los entrenamientos de trials que no muestran mejora despu√©s de una √©poca, ahorrando tiempo computacional.\n",
    "Limpieza de Checkpoints (cleanup_callback): Se implementa un callback inteligente que, despu√©s de cada trial, elimina autom√°ticamente la carpeta de checkpoints del modelo si este no supera al mejor modelo encontrado hasta el momento. Esto previene el consumo excesivo de espacio en disco, un problema com√∫n en HPO.\n",
    "Evaluaci√≥n Final y Empaquetado:\n",
    "\n",
    "Una vez que Optuna completa todos los trials, identifica la mejor configuraci√≥n (best_trial).\n",
    "Carga el modelo campe√≥n desde su checkpoint guardado y utiliza los umbrales de decisi√≥n optimizados que se calcularon para ese trial.\n",
    "Realiza una evaluaci√≥n final y definitiva en el conjunto de datos de prueba (test_enc), que el modelo nunca ha visto, para obtener una medida imparcial de su rendimiento en el mundo real.\n",
    "Finalmente, guarda un paquete de inferencia completo que contiene el modelo entrenado, el tokenizador y el archivo best_thresholds.json. Esto permite que el modelo sea f√°cilmente cargado y utilizado para hacer predicciones en el futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 3: OPTUNA (HPO) + UMBRALES POR CLASE + EVALUACI√ìN FINAL ‚Äî CRITERIO: f1_weighted\n",
    "# ==============================================================================\n",
    "\n",
    "# Requiere que ya existan: MODEL_NAME, LABEL_COLUMNS, tokenizer, train_df, train_enc, val_enc, test_enc, compute_metrics\n",
    "\n",
    "# !pip install optuna -q\n",
    "\n",
    "import os\n",
    "# Desactivar integraciones que bloquean (W&B) y ruido\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
    "\n",
    "import gc, json, math, optuna, numpy as np, torch, shutil # <-- NUEVO: Importamos shutil para borrar carpetas\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "\n",
    "# Info de dispositivo\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available(), \"| #GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU[0]:\", torch.cuda.get_device_name(0))\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# En notebook, el Trainer suele usar 1 GPU. Si usas accelerate/DDP, cambia a torch.cuda.device_count().\n",
    "n_gpus_eff = 1 if torch.cuda.is_available() else 0\n",
    "MAX_PER_DEVICE_BS = 16  # baja a 8 si hay OOM\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tune_thresholds_per_class(probs, y_true, steps=100):\n",
    "    C = probs.shape[1]\n",
    "    thresholds = []\n",
    "    for i in range(C):\n",
    "        best_f, best_th = 0.0, 0.5\n",
    "        p = probs[:, i]\n",
    "        y = y_true[:, i]\n",
    "        for th in np.linspace(0.05, 0.95, steps):\n",
    "            f = f1_score(y, (p >= th).astype(int), zero_division=0)\n",
    "            if f > best_f:\n",
    "                best_f, best_th = f, th\n",
    "        thresholds.append(best_th)\n",
    "    thresholds = np.array(thresholds)\n",
    "    preds = (probs >= thresholds).astype(int)\n",
    "    return thresholds, {\n",
    "        \"f1_macro\":     f1_score(y_true, preds, average=\"macro\",    zero_division=0),\n",
    "        \"f1_micro\":     f1_score(y_true, preds, average=\"micro\",    zero_division=0),\n",
    "        \"f1_weighted\":  f1_score(y_true, preds, average=\"weighted\", zero_division=0),\n",
    "        \"auprc_macro\":  average_precision_score(y_true, probs, average=\"macro\")\n",
    "    }\n",
    "\n",
    "def build_model():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(LABEL_COLUMNS),\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        id2label={i: l for i, l in enumerate(LABEL_COLUMNS)},\n",
    "        label2id={l: i for i, l in enumerate(LABEL_COLUMNS)}\n",
    "    )\n",
    "\n",
    "# pos_weight por desbalance (train_df['labels'])\n",
    "pos_counts = np.array(train_df[\"labels\"].tolist()).sum(axis=0)\n",
    "N = len(train_df)\n",
    "pos_weight = torch.tensor((N - pos_counts) / np.clip(pos_counts, 1, None), dtype=torch.float32)\n",
    "\n",
    "def compute_loss_with_pos_weight(model, inputs, return_outputs=False,**kwargs):\n",
    "    labels = inputs.pop(\"labels\").float()\n",
    "    outputs = model(**inputs)\n",
    "    loss = BCEWithLogitsLoss(pos_weight=pos_weight.to(outputs.logits.device))(outputs.logits, labels)\n",
    "    return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def objective(trial):\n",
    "    # Espacio de b√∫squeda\n",
    "    learning_rate    = trial.suggest_float(\"learning_rate\", 2.5e-5, 5e-5, log=True)\n",
    "    eff_batch_size   = trial.suggest_categorical(\"effective_batch_size\", [16, 32])\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 4, 8)\n",
    "    weight_decay     = trial.suggest_float(\"weight_decay\", 0.0, 0.1) # Mantener este rango amplio\n",
    "    warmup_ratio     = trial.suggest_float(\"warmup_ratio\", 0.0, 0.1) # Acotar un poco, rara vez se necesita m√°s\n",
    "\n",
    "    per_device_bs = min(MAX_PER_DEVICE_BS, eff_batch_size)\n",
    "    den = max(1, per_device_bs * max(1, n_gpus_eff))\n",
    "    grad_accum = max(1, math.ceil(eff_batch_size / den))\n",
    "    print(f\"[Trial {trial.number}] per_device_bs={per_device_bs}, grad_accum={grad_accum}, eff_bs‚âà{per_device_bs*max(1,n_gpus_eff)*grad_accum}\")\n",
    "\n",
    "    output_dir = f\"/kaggle/working/hpo_scibert_uncased/trial_{trial.number}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Guardamos la ruta del directorio para poder acceder a ella en el callback\n",
    "    trial.set_user_attr(\"output_dir\", output_dir) # <-- NUEVO: Guardamos la ruta en los atributos del trial\n",
    "\n",
    "    model = build_model()\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_weighted\",\n",
    "        greater_is_better=True,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=per_device_bs,\n",
    "        per_device_eval_batch_size=max(per_device_bs, 32),\n",
    "        gradient_accumulation_steps=grad_accum,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        logging_steps=50,\n",
    "        save_total_limit=1,\n",
    "        seed=42,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=2, # Reducido a 2 para evitar cuellos de botella en Kaggle\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_enc,\n",
    "        eval_dataset=val_enc,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    "    )\n",
    "    trainer.compute_loss = compute_loss_with_pos_weight\n",
    "    trainer.train()\n",
    "\n",
    "    val_out = trainer.predict(val_enc)\n",
    "    val_probs = sigmoid(val_out.predictions)\n",
    "    val_y = val_out.label_ids\n",
    "    val_thresholds, val_metrics_thr = tune_thresholds_per_class(val_probs, val_y, steps=100)\n",
    "\n",
    "    trial.set_user_attr(\"val_thresholds\", val_thresholds.tolist())\n",
    "    trial.set_user_attr(\"val_metrics_thr\", val_metrics_thr)\n",
    "\n",
    "    del trainer, model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return val_metrics_thr[\"f1_weighted\"]\n",
    "\n",
    "# ==============================================================================\n",
    "# <-- INICIO: NUEVA SECCI√ìN PARA GESTI√ìN DE ALMACENAMIENTO -->\n",
    "# ==============================================================================\n",
    "\n",
    "def cleanup_callback(study: optuna.study.Study, trial: optuna.trial.FrozenTrial):\n",
    "    \"\"\"\n",
    "    Callback para limpiar los checkpoints de los trials que no son el mejor.\n",
    "    Se ejecuta despu√©s de cada trial y borra la carpeta de checkpoints si\n",
    "    el trial reci√©n terminado no es el mejor hasta ahora.\n",
    "    \"\"\"\n",
    "    # Buscamos el directorio del mejor trial hasta el momento\n",
    "    try:\n",
    "        best_trial_dir = study.best_trial.user_attrs.get(\"output_dir\")\n",
    "    except (AttributeError, KeyError):\n",
    "        best_trial_dir = None # A√∫n no hay un mejor trial (p.ej. en el primer trial)\n",
    "\n",
    "    # Buscamos el directorio del trial que acaba de terminar\n",
    "    try:\n",
    "        current_trial_dir = trial.user_attrs.get(\"output_dir\")\n",
    "    except (AttributeError, KeyError):\n",
    "        current_trial_dir = None\n",
    "        \n",
    "    # Si el directorio del trial actual existe y NO es el del mejor trial, lo borramos\n",
    "    if current_trial_dir and current_trial_dir != best_trial_dir and os.path.exists(current_trial_dir):\n",
    "        print(f\"üßπ Limpiando checkpoint del trial {trial.number} (no es el mejor). Directorio: {current_trial_dir}\")\n",
    "        shutil.rmtree(current_trial_dir)\n",
    "\n",
    "# ==============================================================================\n",
    "# <-- FIN: NUEVA SECCI√ìN -->\n",
    "# ==============================================================================\n",
    "\n",
    "# Ejecutar estudio\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"scibert_uncased_hpo\",\n",
    "                            pruner=optuna.pruners.MedianPruner(n_startup_trials=2))\n",
    "study.optimize(objective, n_trials=10, callbacks=[cleanup_callback]) # <-- NUEVO: A√±adimos el callback\n",
    "\n",
    "print(\"\\nMejores hiperpar√°metros:\", study.best_params)\n",
    "print(\"Mejor F1_weighted (val, con thresholds):\", study.best_value)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "best_dir = best_trial.user_attrs[\"output_dir\"]\n",
    "best_thresholds = np.array(best_trial.user_attrs[\"val_thresholds\"])\n",
    "print(\"Checkpoint del mejor trial:\", best_dir)\n",
    "print(\"Umbrales del mejor trial:\", best_thresholds)\n",
    "\n",
    "# Evaluaci√≥n final en TEST con umbrales del mejor trial\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    best_dir,\n",
    "    num_labels=len(LABEL_COLUMNS),\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label={i: l for i, l in enumerate(LABEL_COLUMNS)},\n",
    "    label2id={l: i for i, l in enumerate(LABEL_COLUMNS)}\n",
    ")\n",
    "\n",
    "best_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/scibert_uncased_best_final\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "best_trainer = Trainer(model=best_model, args=best_args, eval_dataset=test_enc)\n",
    "\n",
    "test_out = best_trainer.predict(test_enc)\n",
    "test_probs = sigmoid(test_out.predictions)\n",
    "test_y = test_out.label_ids\n",
    "test_preds_thr = (test_probs >= best_thresholds).astype(int)\n",
    "\n",
    "test_metrics = {\n",
    "    \"f1_macro\":    f1_score(test_y, test_preds_thr, average=\"macro\",    zero_division=0),\n",
    "    \"f1_micro\":    f1_score(test_y, test_preds_thr, average=\"micro\",    zero_division=0),\n",
    "    \"f1_weighted\": f1_score(test_y, test_preds_thr, average=\"weighted\", zero_division=0),\n",
    "    \"auprc_macro\": average_precision_score(test_y, test_probs, average=\"macro\")\n",
    "}\n",
    "print(\"\\nüß™ M√©tricas en TEST con umbrales ajustados (mejor trial):\")\n",
    "print(test_metrics)\n",
    "\n",
    "# Guardar mejor modelo, tokenizador y umbrales\n",
    "final_dir = \"/kaggle/working/scibert_uncased_hpo_best\"\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "\n",
    "best_model.save_pretrained(final_dir) # <-- Guarda el modelo\n",
    "tokenizer.save_pretrained(final_dir) # <-- A√ëADIR ESTA L√çNEA para guardar el tokenizador\n",
    "\n",
    "with open(os.path.join(final_dir, \"best_thresholds.json\"), \"w\") as f:\n",
    "    json.dump({\"labels\": LABEL_COLUMNS, \"thresholds\": best_thresholds.tolist()}, f, indent=2) # <-- Guarda los umbrales\n",
    "print(f\"\\n‚úÖ Paquete de inferencia completo guardado en: {final_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizaci√≥n de Hiperpar√°metros: Resultados y Mejor Configuraci√≥n\n",
    "Se llev√≥ a cabo un proceso de optimizaci√≥n de hiperpar√°metros (HPO) para encontrar la configuraci√≥n √≥ptima del modelo SciBERT en la tarea de clasificaci√≥n. Se evaluaron un total de 10 combinaciones (trials) utilizando la m√©trica de F1-Score Ponderado (Weighted F1-Score) sobre el conjunto de validaci√≥n.\n",
    "\n",
    "Resultados Clave\n",
    "Tras el an√°lisis de los 10 trials ejecutados, el Trial 6 emergi√≥ como el de mejor rendimiento, superando a todas las dem√°s configuraciones evaluadas.\n",
    "\n",
    "Mejor Trial: Trial 6\n",
    "Valor de la M√©trica (Weighted F1-Score): 0.9623\n",
    "Hiperpar√°metros del Mejor Modelo (Trial 6)\n",
    "La configuraci√≥n de hiperpar√°metros que produjo este resultado fue la siguiente:\n",
    "\n",
    "Hiperpar√°metro\tValor\tDescripci√≥n\n",
    "learning_rate\t4.748e-05\tTasa de aprendizaje para el optimizador AdamW.\n",
    "effective_batch_size\t16\tTama√±o de lote efectivo para el entrenamiento.\n",
    "num_train_epochs\t7\tN√∫mero de √©pocas completas de entrenamiento.\n",
    "weight_decay\t0.00956\tCoeficiente de regularizaci√≥n L2 para prevenir el sobreajuste.\n",
    "warmup_ratio\t0.0196\tProporci√≥n de pasos de \"calentamiento\" para la tasa de aprendizaje.\n",
    "Conclusi√≥n\n",
    "El modelo final ser√° entrenado utilizando la configuraci√≥n del Trial 6, ya que ha demostrado ser la m√°s efectiva durante la fase de experimentaci√≥n y optimizaci√≥n. Los archivos de este modelo servir√°n como base para la evaluaci√≥n final en el conjunto de prueba y para el despliegue de la soluci√≥n."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8133231,
     "sourceId": 12858791,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
